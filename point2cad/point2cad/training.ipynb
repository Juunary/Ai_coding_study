{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aea7b1f",
   "metadata": {},
   "source": [
    "### 데이터 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======1단계: 입력 폴더 경로 설정\n",
    "path_in = r\"..\\assets\\ply\\impeller\\3wings\"   # Windows 경로 예시\n",
    "downsample_N = 20000         # 각각 포인트 샘플링 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446de869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from pathlib import Path\n",
    "\n",
    "#======= 2단계: 단일 PLY 파일 → 점군으로 변환 후 .xyz 저장 =======\n",
    "def convert_to_xyz(path_in: str, downsample_N: int = 20000):\n",
    "    \"\"\"\n",
    "    단일 PLY 파일 → 점군으로 변환 후 .xyz 저장\n",
    "    \"\"\"\n",
    "    mesh = o3d.io.read_triangle_mesh(path_in)\n",
    "    if mesh.has_triangles():\n",
    "        pcd = mesh.sample_points_poisson_disk(number_of_points=downsample_N)\n",
    "    else:\n",
    "        pcd = o3d.io.read_point_cloud(path_in)\n",
    "        if len(pcd.points) > downsample_N:\n",
    "            pts = np.asarray(pcd.points)\n",
    "            idx = np.random.choice(len(pts), downsample_N, replace=False)\n",
    "            pcd.points = o3d.utility.Vector3dVector(pts[idx])\n",
    "\n",
    "    out_path = Path(path_in).with_name(Path(path_in).stem + \"_point.xyz\")\n",
    "    np.savetxt(out_path, np.asarray(pcd.points), fmt=\"%.6f\")\n",
    "    print(f\"[OK] {path_in} → {out_path} (N={len(pcd.points)})\")\n",
    "\n",
    "\n",
    "# 지정한 폴더에 있는 모든 .ply 파일 변환\n",
    "folder = Path(path_in)\n",
    "if not folder.is_dir():\n",
    "    print(f\"[ERR] 폴더 경로가 아님: {folder}\")\n",
    "else:\n",
    "    ply_files = list(folder.glob(\"*.ply\"))\n",
    "    if not ply_files:\n",
    "        print(f\"[INFO] {folder} 안에 .ply 파일이 없습니다.\")\n",
    "    else:\n",
    "        for ply in ply_files:\n",
    "            convert_to_xyz(str(ply), downsample_N=downsample_N)\n",
    "        print(f\"[DONE] 총 {len(ply_files)} 개 파일 변환 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== 3단계: .xyz 파일 → .xyzc 파일로 변환 ======\n",
    "def convert_xyz_to_xyzc(xyz_path: Path):\n",
    "    \"\"\"\n",
    "    파일명에서 숫자 추출 → 라벨로 추가 → .xyzc 저장\n",
    "    예: impeller_b3_010_point.xyz → 라벨 = 10.00000000\n",
    "    \"\"\"\n",
    "    # 파일 이름에서 숫자 추출\n",
    "    m = re.search(r\"_(\\d+)\", xyz_path.stem)\n",
    "    if not m:\n",
    "        print(f\"[SKIP] 라벨 숫자 추출 실패: {xyz_path.name}\")\n",
    "        return None\n",
    "    \n",
    "    label_value = float(m.group(1))  # \"010\" → 10.0\n",
    "\n",
    "    # xyz 불러오기\n",
    "    xyz = np.loadtxt(xyz_path)\n",
    "    if xyz.ndim == 1:\n",
    "        xyz = xyz.reshape(-1, 3)\n",
    "\n",
    "    # 라벨 추가\n",
    "    label_column = np.full((xyz.shape[0], 1), label_value)\n",
    "    xyzc = np.hstack((xyz, label_column))\n",
    "\n",
    "    # 저장\n",
    "    out_path = xyz_path.with_suffix(\".xyzc\")\n",
    "    np.savetxt(out_path, xyzc, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "    print(f\"[OK] {xyz_path.name} → {out_path.name} (label={label_value:.8f})\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ====== 폴더 내 모든 _point.xyz 파일 처리 ======\n",
    "folder = Path(path_in)\n",
    "xyz_files = list(folder.glob(\"*_point.xyz\"))\n",
    "\n",
    "if not xyz_files:\n",
    "    print(f\"[INFO] {folder} 안에 '_point.xyz' 파일이 없습니다.\")\n",
    "else:\n",
    "    for xyz_file in xyz_files:\n",
    "        convert_xyz_to_xyzc(xyz_file)\n",
    "\n",
    "    print(f\"[DONE] 총 {len(xyz_files)} 개 파일 변환 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b735c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== 4단계: 모든 XYZC 파일 병합 ======\n",
    "\n",
    "output_file = Path(path_in) / \"merged_all.xyzc\"  # 최종 합쳐진 파일명\n",
    "\n",
    "# 폴더 내 모든 .xyzc 찾기\n",
    "folder = Path(path_in)\n",
    "xyzc_files = list(folder.glob(\"*.xyzc\"))\n",
    "\n",
    "if not xyzc_files:\n",
    "    print(f\"[INFO] {folder} 안에 .xyzc 파일이 없습니다.\")\n",
    "else:\n",
    "    merged_data = []\n",
    "    for f in xyzc_files:\n",
    "        try:\n",
    "            data = np.loadtxt(f)\n",
    "            if data.ndim == 1:  # 한 줄짜리 방지\n",
    "                data = data.reshape(1, -1)\n",
    "            merged_data.append(data)\n",
    "            print(f\"[READ] {f.name} (N={len(data)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {f.name} 불러오기 실패: {e}\")\n",
    "\n",
    "    if merged_data:\n",
    "        merged_data = np.vstack(merged_data)\n",
    "        np.savetxt(output_file, merged_data, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "        print(f\"[DONE] {len(xyzc_files)} 개 파일 합침 → {output_file} (총 {len(merged_data)} 포인트)\")\n",
    "    else:\n",
    "        print(\"[INFO] 합칠 데이터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_xyzc(path):\n",
    "    data = np.loadtxt(path)\n",
    "    if data.shape[1] != 4:\n",
    "        raise ValueError(\"Input file must have 4 columns: x, y, z, label\")\n",
    "    return data\n",
    "\n",
    "def print_layer_distribution(label_array):\n",
    "    total = len(label_array)\n",
    "    label_counts = Counter(label_array)\n",
    "    print(\"📊 Layer distribution (by percentage):\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        percent = 100.0 * count / total\n",
    "        print(f\"  Layer {label:.6f}: {count} points ({percent:.2f}%)\")\n",
    "    print(\"\")\n",
    "\n",
    "def visualize_point_cloud(data):\n",
    "    x, y, z, label = data[:,0], data[:,1], data[:,2], data[:,3]\n",
    "    print_layer_distribution(label)\n",
    "\n",
    "    unique_labels = np.unique(label)\n",
    "    label_to_color = {l: plt.cm.tab20(i / max(len(unique_labels)-1, 1)) \n",
    "                      for i, l in enumerate(unique_labels)}\n",
    "    colors = np.array([label_to_color[l] for l in label])\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colors, s=1)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('Point Cloud Visualization by Layer')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load and visualize merged point cloud\n",
    "merged_xyzc_path = output_file  # already defined in your notebook\n",
    "data = load_xyzc(merged_xyzc_path)\n",
    "visualize_point_cloud(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708449f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Jupyter 한 셀에서 모두 해결 ====\n",
    "# 1) 필수 임포트\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # <- Pylance 'optim' undefined 해결\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import mode\n",
    "\n",
    "def normalize_points(points):\n",
    "    # (N,3) → 원점 기준, 단위 스케일\n",
    "    c = points.mean(0)\n",
    "    p = points - c\n",
    "    s = np.max(np.linalg.norm(p, axis=1))\n",
    "    p = p / (s + 1e-8)\n",
    "    return p.astype(np.float32), (c, s)\n",
    "\n",
    "# 인덱스 보존형 증강(라벨 손실 없음): jitter/작은 회전/이동/스케일\n",
    "def aug_index_preserving(p, jitter_sigma=0.002, jitter_clip=0.01,\n",
    "                         rot_deg=2, trans_mag=0.005, scale_jitter=0.02):\n",
    "    q = p.copy()\n",
    "    # jitter\n",
    "    n = np.clip(np.random.normal(0, jitter_sigma, q.shape), -jitter_clip, jitter_clip)\n",
    "    q += n\n",
    "    # small rotation (z축 예시; 필요시 x/y도 추가)\n",
    "    th = np.deg2rad(np.random.uniform(-rot_deg, rot_deg))\n",
    "    c, s = np.cos(th), np.sin(th)\n",
    "    Rz = np.array([[c,-s,0],[s,c,0],[0,0,1]], np.float32)\n",
    "    q = (q @ Rz.T).astype(np.float32)\n",
    "    # small translation\n",
    "    t = np.random.uniform(-trans_mag, trans_mag, (1,3)).astype(np.float32)\n",
    "    q = q + t\n",
    "    # small scaling\n",
    "    sc = 1.0 + np.random.uniform(-scale_jitter, scale_jitter)\n",
    "    q = q * sc\n",
    "    return q\n",
    "\n",
    "def knn_smooth(points, preds, k=16):\n",
    "    k = int(min(k, len(points)))\n",
    "    idx = cKDTree(points).query(points, k=k)[1]\n",
    "    smoothed = mode(preds[idx], axis=1, keepdims=False).mode\n",
    "    return np.asarray(smoothed, dtype=preds.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_ROOT = os.path.abspath(\".\")  # 예: 프로젝트 루트\n",
    "if SRC_ROOT not in sys.path:\n",
    "    sys.path.append(SRC_ROOT)\n",
    "\n",
    "try:\n",
    "    # 보통 파일 경로는 src/PointNet_seg.py 형태\n",
    "    from src.PointNet_seg import PointNetSeg  # <- Pylance 'PointNetSeg' undefined 해결\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"PointNetSeg를 import하지 못했습니다. \"\n",
    "        \"노트북 작업 디렉토리 기준으로 'src/PointNet_seg.py'가 있는지 확인하고, \"\n",
    "        \"경로가 다르면 SRC_ROOT를 맞춰주세요.\"\n",
    "    ) from e\n",
    "# 3) FocalLoss 정의 (학습 스크립트와 동일/호환)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (B,C) 또는 (N,C) 형태, targets: (B,) 또는 (N,)\n",
    "        ce = self.ce(logits, targets)             # (N,)\n",
    "        pt = torch.exp(-ce)                       # p = exp(-CE)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce      # focal\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "def train_impeller_robust(xyzc_path, model_out, lr, stop_loss, device, sample_size,\n",
    "                          lambda_consis=0.5, epochs=200):\n",
    "    print(f\"[Train-Robust] Using device: {device}\")\n",
    "    data = np.loadtxt(xyzc_path).astype(np.float32)\n",
    "    points = data[:, :3]\n",
    "    orig_labels = data[:, 3].astype(np.int64)\n",
    "\n",
    "    # 0) 정규화\n",
    "    points, _ = normalize_points(points)\n",
    "\n",
    "    # 1) 라벨 매핑\n",
    "    unique_labels = np.unique(orig_labels)\n",
    "    label_to_idx = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    labels = np.array([label_to_idx[l] for l in orig_labels], dtype=np.int64)\n",
    "\n",
    "    # 2) 모델/손실/옵티마이저\n",
    "    num_classes = len(unique_labels)\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "\n",
    "    # class-imbalance 가중치(있으면 그대로 사용)\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    inv_freq = 1.0 / (counts + 1e-6)\n",
    "    weights = inv_freq / np.sum(inv_freq) * num_classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Focal + Consistency\n",
    "    criterion_sup = FocalLoss(gamma=2.0, weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    best_loss = float('inf'); best_state = None; full_loss_history = []\n",
    "    N = points.shape[0]; steps_per_epoch = max(1, N // sample_size)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        cycle_loss = 0.0\n",
    "        # 점진적 램프업(초반엔 감독 위주, 후반엔 일관성 비중 ↑)\n",
    "        lam = lambda_consis * min(1.0, epoch / (epochs*0.3))\n",
    "\n",
    "        for _ in range(steps_per_epoch):\n",
    "            idx = np.random.choice(N, sample_size, replace=False)\n",
    "            x_clean = points[idx]          # (B,3)\n",
    "            y_batch = labels[idx]          # (B,)\n",
    "\n",
    "            # 인덱스 유지하는 증강만 사용(라벨 매칭 깨지지 않도록)\n",
    "            x_aug = aug_index_preserving(x_clean)\n",
    "\n",
    "            x_clean_t = torch.from_numpy(x_clean.T).unsqueeze(0).to(device)  # (1,3,B)\n",
    "            x_aug_t   = torch.from_numpy(x_aug.T).unsqueeze(0).to(device)\n",
    "            y_t       = torch.from_numpy(y_batch).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_clean = model(x_clean_t).squeeze(0).permute(1,0)  # (B,C)\n",
    "            logits_aug   = model(x_aug_t).squeeze(0).permute(1,0)    # (B,C)\n",
    "\n",
    "            # train 루프 내부\n",
    "            loss_sup = criterion_sup(logits_clean, y_t)\n",
    "\n",
    "            beta = 0.2  # 0.1~0.3 권장\n",
    "            loss_sup_aug = criterion_sup(logits_aug, y_t)\n",
    "\n",
    "            p_clean = F.softmax(logits_clean, dim=1)\n",
    "            p_aug   = F.softmax(logits_aug,   dim=1)\n",
    "            loss_cons = F.mse_loss(p_clean, p_aug)\n",
    "\n",
    "            loss = loss_sup + beta*loss_sup_aug + lam*loss_cons\n",
    "\n",
    "            loss.backward(); optimizer.step()\n",
    "            cycle_loss += float(loss.item())\n",
    "\n",
    "        avg_loss = cycle_loss / steps_per_epoch\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch}/{epochs}  Loss: {avg_loss:.6f}  (sup={loss_sup.item():.6f}, cons={loss_cons.item():.6f}, lam={lam:.3f})\")\n",
    "\n",
    "        # 10epoch마다 full-loss(정규화된 전체 포인트 기준)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_full = torch.from_numpy(points.T).unsqueeze(0).to(device)\n",
    "                y_full = torch.from_numpy(labels).to(device)\n",
    "                logits_full = model(x_full).squeeze(0).permute(1,0)\n",
    "                full_loss = criterion_sup(logits_full, y_full).item()\n",
    "            model.train()\n",
    "            full_loss_history.append(full_loss)\n",
    "            print(f\"  FullLoss: {full_loss:.6f}\")\n",
    "\n",
    "            if full_loss < best_loss:\n",
    "                best_loss = full_loss; best_state = model.state_dict()\n",
    "                save_dir = os.path.dirname(model_out) or \".\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                torch.save({'state_dict': best_state, 'unique_labels': unique_labels.tolist()},\n",
    "                           os.path.join(save_dir, 'best_'+os.path.basename(model_out)))\n",
    "                print(f\"  [Best] FullLoss improved → {best_loss:.6f}\")\n",
    "\n",
    "            # 간단 early stop (원하면 검증 셋 분리 추천)\n",
    "            if len(full_loss_history)>=5 and all(l <= stop_loss for l in full_loss_history[-5:]):\n",
    "                print(f\"  [EarlyStop] last 5 FullLoss ≤ {stop_loss}\")\n",
    "                break\n",
    "\n",
    "    # 최종 저장\n",
    "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
    "    final_state = best_state if best_state is not None else model.state_dict()\n",
    "    torch.save({'state_dict': final_state,\n",
    "                'unique_labels': unique_labels.tolist()}, model_out)\n",
    "    print(f\"[Saved] Model → {model_out}  (best_loss={best_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c18812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_impeller_robust(xyz_path, model_path, device, tta=5, knn_k=16,\n",
    "                          small_jitter=0.001, small_rot_deg=2, small_trans=0.003):\n",
    "    print(f\"[Infer-Robust] Using {device}\")\n",
    "    data = np.loadtxt(xyz_path).astype(np.float32)\n",
    "    orig_points = data[:, :3]                         # 원본 보관\n",
    "    points, (c, s) = normalize_points(orig_points)    # 정규화\n",
    "\n",
    "    \n",
    "\n",
    "    # 모델 로드\n",
    "    chk = torch.load(model_path, map_location=device)\n",
    "    inv_map = {i:l for i,l in enumerate(chk['unique_labels'])}\n",
    "    num_classes = len(chk['unique_labels'])\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(chk['state_dict']); model.eval()\n",
    "\n",
    "    # TTA: 작은 노이즈/회전/이동 여러 번 → 확률 평균\n",
    "    with torch.no_grad():\n",
    "        acc_probs = None\n",
    "        for _ in range(tta):\n",
    "            p_aug = aug_index_preserving(points,\n",
    "                                         jitter_sigma=small_jitter, jitter_clip=3*small_jitter,\n",
    "                                         rot_deg=small_rot_deg, trans_mag=small_trans, scale_jitter=0.0)\n",
    "            x = torch.from_numpy(p_aug.T).unsqueeze(0).to(device)\n",
    "            logits = model(x).squeeze(0)          # (C,N)\n",
    "            probs = torch.softmax(logits, dim=0)  # (C,N)\n",
    "            acc_probs = probs if acc_probs is None else (acc_probs + probs)\n",
    "        probs_avg = acc_probs / float(tta)\n",
    "        preds = probs_avg.argmax(dim=0).cpu().numpy()\n",
    "\n",
    "    # KNN 스무딩\n",
    "    preds = knn_smooth(points, preds, k=knn_k)\n",
    "\n",
    "    # 저장 (원본 스케일 좌표를 쓰고 싶으면 정규화 이전 좌표 따로 보관하세요)\n",
    "    out_xyz = points * s + c                          # 복원!\n",
    "    out = np.hstack([out_xyz, np.array([inv_map[int(i)] for i in preds])[:, None]])\n",
    "    out_path = xyz_path.replace('.xyz','_impeller_pred.xyzc')\n",
    "    np.savetxt(out_path, out, fmt='%.6f'); print(f\"Saved → {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_impeller_robust(\n",
    "    xyzc_path=\"../assets/ply/impeller/3wings/impeller_b3_merged_all.xyzc\",\n",
    "    model_out=\"./models/impeller_seg_robust_b3.pth\",\n",
    "    lr=1e-4, stop_loss=0.001,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    sample_size=2000,\n",
    "    lambda_consis=0.2, epochs=2000\n",
    ")\n",
    "\n",
    "# Infer\n",
    "infer_impeller_robust(\n",
    "    xyz_path=\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Ai_coding_study\\\\point2cad\\\\assets\\\\xyz\\\\Impeller_b3_gpu.xyz\",\n",
    "    model_path=\"./models/impeller_seg_robust_b3.pth\",\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    tta=5, knn_k=16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Jupyter 한 셀에서 모두 해결 (Robust PointNetSeg) ====\n",
    "# 필수 임포트\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import mode\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 유틸\n",
    "# ------------------------------------------------------------\n",
    "def normalize_points(points: np.ndarray):\n",
    "    \"\"\"(N,3) → 원점 기준 단위 스케일 정규화, (정규화된 점, (중심, 스케일)) 반환\"\"\"\n",
    "    c = points.mean(0)\n",
    "    p = points - c\n",
    "    s = np.max(np.linalg.norm(p, axis=1))\n",
    "    p = p / (s + 1e-8)\n",
    "    return p.astype(np.float32), (c, s)\n",
    "\n",
    "def aug_index_preserving(p: np.ndarray,\n",
    "                         jitter_sigma=0.002, jitter_clip=0.01,\n",
    "                         rot_deg=2.0, trans_mag=0.005, scale_jitter=0.02):\n",
    "    \"\"\"인덱스 보존형 증강(라벨 손실 없음)\"\"\"\n",
    "    q = p.copy()\n",
    "    # jitter\n",
    "    n = np.clip(np.random.normal(0, jitter_sigma, q.shape), -jitter_clip, jitter_clip)\n",
    "    q += n\n",
    "    # small rotation (z축, 필요 시 x/y도 추가 가능)\n",
    "    th = np.deg2rad(np.random.uniform(-rot_deg, rot_deg))\n",
    "    c, s = np.cos(th), np.sin(th)\n",
    "    Rz = np.array([[c,-s,0],[s,c,0],[0,0,1]], np.float32)\n",
    "    q = (q @ Rz.T).astype(np.float32)\n",
    "    # small translation\n",
    "    t = np.random.uniform(-trans_mag, trans_mag, (1,3)).astype(np.float32)\n",
    "    q = q + t\n",
    "    # small scaling\n",
    "    sc = 1.0 + np.random.uniform(-scale_jitter, scale_jitter)\n",
    "    q = q * sc\n",
    "    return q\n",
    "\n",
    "def knn_smooth(points: np.ndarray, preds: np.ndarray, k=16):\n",
    "    \"\"\"KNN 다수결 라벨 스무딩\"\"\"\n",
    "    k = int(min(k, len(points)))\n",
    "    idx = cKDTree(points).query(points, k=k)[1]\n",
    "    smoothed = mode(preds[idx], axis=1, keepdims=False).mode\n",
    "    return np.asarray(smoothed, dtype=preds.dtype)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 모델 임포트\n",
    "# ------------------------------------------------------------\n",
    "SRC_ROOT = os.path.abspath(\".\")  # 예: 프로젝트 루트\n",
    "if SRC_ROOT not in sys.path:\n",
    "    sys.path.append(SRC_ROOT)\n",
    "\n",
    "try:\n",
    "    # 보통 파일 경로는 src/PointNet_seg.py 형태\n",
    "    from src.PointNet_seg import PointNetSeg\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"PointNetSeg를 import하지 못했습니다. \"\n",
    "        \"노트북 작업 디렉토리 기준으로 'src/PointNet_seg.py'가 있는지 확인하고, \"\n",
    "        \"경로가 다르면 SRC_ROOT를 맞춰주세요.\"\n",
    "    ) from e\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 손실 정의\n",
    "# ------------------------------------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (N,C), targets: (N,)\n",
    "        ce = self.ce(logits, targets)             # (N,)\n",
    "        pt = torch.exp(-ce)                       # p = exp(-CE)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce      # focal\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (옵션) EMA Teacher & KL 일관성\n",
    "# ------------------------------------------------------------\n",
    "import copy\n",
    "def create_ema(model):\n",
    "    ema = copy.deepcopy(model).eval()\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return ema\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(student, teacher, mu=0.999):\n",
    "    for ps, pt in zip(student.parameters(), teacher.parameters()):\n",
    "        pt.data.mul_(mu).add_(ps.data, alpha=1-mu)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 학습 루틴\n",
    "# ------------------------------------------------------------\n",
    "def train_impeller_robust(xyzc_path,\n",
    "                          model_out=\"./models/impeller_seg_robust.pth\",\n",
    "                          lr=1e-4, stop_loss=0.001,\n",
    "                          device=None,\n",
    "                          sample_size=2000,\n",
    "                          lambda_consis=0.2,   # 일관성 손실 가중치(최대)\n",
    "                          beta_aug=0.2,        # 증강 배치 감독 가중치\n",
    "                          epochs=2000,\n",
    "                          use_label_smoothing=False, label_smoothing=0.05,\n",
    "                          use_ema_kld=False, ema_mu=0.999, cons_temp=2.0,\n",
    "                          save_curve=True):\n",
    "    \"\"\"\n",
    "    - 증강 배치에도 감독 손실 추가(beta_aug)\n",
    "    - 일관성 손실 램프업(lambda_consis * min(1, epoch/(epochs*0.3)))\n",
    "    - (옵션) EMA teacher + KL 일관성\n",
    "    - CosineAnnealingLR 스케줄\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[Train-Robust] Using device: {device}\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    data = np.loadtxt(xyzc_path).astype(np.float32)\n",
    "    points = data[:, :3]\n",
    "    orig_labels = data[:, 3].astype(np.int64)\n",
    "\n",
    "    # 정규화\n",
    "    points, _ = normalize_points(points)\n",
    "\n",
    "    # 라벨 인덱스화\n",
    "    unique_labels = np.unique(orig_labels)\n",
    "    label_to_idx = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    labels = np.array([label_to_idx[l] for l in orig_labels], dtype=np.int64)\n",
    "\n",
    "    # 모델/손실/옵티마이저\n",
    "    num_classes = len(unique_labels)\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "\n",
    "    # 클래스 불균형 가중치\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    inv_freq = 1.0 / (counts + 1e-6)\n",
    "    weights = inv_freq / np.sum(inv_freq) * num_classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 감독 손실 선택\n",
    "    if use_label_smoothing:\n",
    "        criterion_sup = nn.CrossEntropyLoss(weight=class_weights,\n",
    "                                            label_smoothing=label_smoothing)\n",
    "    else:\n",
    "        criterion_sup = FocalLoss(gamma=2.0, weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # (옵션) EMA teacher\n",
    "    ema_model = create_ema(model) if use_ema_kld else None\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    full_loss_history = []\n",
    "\n",
    "    N = points.shape[0]\n",
    "    steps_per_epoch = max(1, int(np.ceil(N / max(1, sample_size))))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        cycle_loss = 0.0\n",
    "        # 일관성 램프업 (초반 30% 구간)\n",
    "        lam = lambda_consis * min(1.0, epoch / (epochs * 0.3))\n",
    "\n",
    "        # (선택) 후반부 약간 완화\n",
    "        if epoch > 0.8 * epochs:\n",
    "            lam *= 0.5\n",
    "\n",
    "        # 증강 강도도 후반에 살짝 감소\n",
    "        cur_sigma = float(np.interp(epoch, [1, 0.7*epochs, epochs],\n",
    "                                    [0.002, 0.002, 0.001]))\n",
    "        cur_rot   = float(np.interp(epoch, [1, 0.7*epochs, epochs],\n",
    "                                    [2.0,   2.0,   1.0  ]))\n",
    "        cur_trans = 0.003  # 고정 (필요시 스케줄링)\n",
    "\n",
    "        for _ in range(steps_per_epoch):\n",
    "            # 샘플링 (데이터가 sample_size보다 적으면 replace=True)\n",
    "            replace_flag = sample_size > N\n",
    "            idx = np.random.choice(N, sample_size, replace=replace_flag)\n",
    "            x_clean = points[idx]      # (B,3)\n",
    "            y_batch = labels[idx]      # (B,)\n",
    "\n",
    "            # 인덱스 보존 증강\n",
    "            x_aug = aug_index_preserving(\n",
    "                x_clean,\n",
    "                jitter_sigma=cur_sigma, jitter_clip=3*cur_sigma,\n",
    "                rot_deg=cur_rot, trans_mag=cur_trans, scale_jitter=0.01\n",
    "            )\n",
    "\n",
    "            x_clean_t = torch.from_numpy(x_clean.T).unsqueeze(0).to(device)  # (1,3,B)\n",
    "            x_aug_t   = torch.from_numpy(x_aug.T).unsqueeze(0).to(device)\n",
    "            y_t       = torch.from_numpy(y_batch).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            logits_clean = model(x_clean_t).squeeze(0).permute(1, 0)  # (B,C)\n",
    "            logits_aug   = model(x_aug_t).squeeze(0).permute(1, 0)    # (B,C)\n",
    "\n",
    "            # 감독 손실 (clean)\n",
    "            loss_sup = criterion_sup(logits_clean, y_t)\n",
    "\n",
    "            # 증강 배치 감독(약하게)\n",
    "            loss_sup_aug = criterion_sup(logits_aug, y_t) if beta_aug > 0 else 0.0\n",
    "\n",
    "            # 일관성 손실\n",
    "            if use_ema_kld:\n",
    "                # EMA teacher KL( student||teacher )\n",
    "                with torch.no_grad():\n",
    "                    t_logits = ema_model(x_aug_t).squeeze(0).permute(1,0) / cons_temp\n",
    "                    t_prob   = F.softmax(t_logits, dim=1)\n",
    "                s_logits = logits_aug / cons_temp\n",
    "                log_prob = F.log_softmax(s_logits, dim=1)\n",
    "                loss_cons = F.kl_div(log_prob, t_prob, reduction='batchmean') * (cons_temp * cons_temp)\n",
    "            else:\n",
    "                # 단순 MSE(consistency)\n",
    "                p_clean = F.softmax(logits_clean, dim=1)\n",
    "                p_aug   = F.softmax(logits_aug,   dim=1)\n",
    "                loss_cons = F.mse_loss(p_clean, p_aug)\n",
    "\n",
    "            # 총손실\n",
    "            if isinstance(loss_sup_aug, float):\n",
    "                loss = loss_sup + lam * loss_cons\n",
    "            else:\n",
    "                loss = loss_sup + beta_aug * loss_sup_aug + lam * loss_cons\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # EMA 업데이트\n",
    "            if ema_model is not None:\n",
    "                update_ema(model, ema_model, mu=ema_mu)\n",
    "\n",
    "            cycle_loss += float(loss.item())\n",
    "\n",
    "        avg_loss = cycle_loss / steps_per_epoch\n",
    "        scheduler.step()\n",
    "        # 모니터링: 마지막 스텝의 loss 항들 출력 (대표값)\n",
    "        cons_val = float(loss_cons.detach().cpu()) if torch.is_tensor(loss_cons) else float(loss_cons)\n",
    "        print(f\"Epoch {epoch}/{epochs}  Loss: {avg_loss:.6f}  (sup={float(loss_sup.detach().cpu()):.6f}, cons={cons_val:.6f}, lam={lam:.3f})\")\n",
    "\n",
    "        # 10 epoch마다 전체 평가(정규화 좌표 기준)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_full = torch.from_numpy(points.T).unsqueeze(0).to(device)\n",
    "                y_full = torch.from_numpy(labels).to(device)\n",
    "                logits_full = model(x_full).squeeze(0).permute(1,0)\n",
    "                full_loss = criterion_sup(logits_full, y_full).item()\n",
    "            model.train()\n",
    "\n",
    "            full_loss_history.append(full_loss)\n",
    "            print(f\"  FullLoss: {full_loss:.6f}\")\n",
    "\n",
    "            # best 저장\n",
    "            save_dir = os.path.dirname(model_out) or \".\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            if full_loss < best_loss:\n",
    "                best_loss = full_loss\n",
    "                best_state = model.state_dict()\n",
    "                torch.save({'state_dict': best_state, 'unique_labels': unique_labels.tolist()},\n",
    "                           os.path.join(save_dir, 'best_'+os.path.basename(model_out)))\n",
    "                print(f\"  [Best] FullLoss improved → {best_loss:.6f}\")\n",
    "\n",
    "            # 간단 early stop\n",
    "            if len(full_loss_history) >= 5 and all(l <= stop_loss for l in full_loss_history[-5:]):\n",
    "                print(f\"  [EarlyStop] last 5 FullLoss ≤ {stop_loss}\")\n",
    "                break\n",
    "\n",
    "    # 최종 저장\n",
    "    os.makedirs(os.path.dirname(model_out) or \".\", exist_ok=True)\n",
    "    final_state = best_state if best_state is not None else model.state_dict()\n",
    "    torch.save({'state_dict': final_state, 'unique_labels': unique_labels.tolist()}, model_out)\n",
    "    print(f\"[Saved] Model → {model_out}  (best_loss={best_loss:.6f})\")\n",
    "\n",
    "    # Loss curve 저장(옵션)\n",
    "    if save_curve and len(full_loss_history) > 0:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(full_loss_history, marker='o')\n",
    "        plt.title('Full Loss History (every 10 epochs)')\n",
    "        plt.xlabel('Eval # (x10 epochs)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        curve_path = os.path.join(os.path.dirname(model_out) or \".\", \"full_loss_curve.png\")\n",
    "        plt.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"[Saved] Loss curve → {curve_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 추론 루틴\n",
    "# ------------------------------------------------------------\n",
    "def infer_impeller_robust(xyz_path, model_path, device=None,\n",
    "                          tta=5, knn_k=16,\n",
    "                          small_jitter=0.001, small_rot_deg=2, small_trans=0.003):\n",
    "    \"\"\"\n",
    "    - 입력 xyz를 정규화 → TTA(작은 증강 여러 번) 확률 평균 → KNN 스무딩 → 원본 스케일 복원 → xyzc 저장\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[Infer-Robust] Using {device}\")\n",
    "\n",
    "    data = np.loadtxt(xyz_path).astype(np.float32)\n",
    "    orig_points = data[:, :3]\n",
    "    points, (c, s) = normalize_points(orig_points)  # 정규화 (한 번만)\n",
    "\n",
    "    # 모델 로드\n",
    "    chk = torch.load(model_path, map_location=device)\n",
    "    inv_map = {i: l for i, l in enumerate(chk['unique_labels'])}\n",
    "    num_classes = len(chk['unique_labels'])\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(chk['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # TTA: 작은 노이즈/회전/이동 여러 번 → 확률 평균\n",
    "    with torch.no_grad():\n",
    "        acc_probs = None\n",
    "        for _ in range(tta):\n",
    "            p_aug = aug_index_preserving(points,\n",
    "                                         jitter_sigma=small_jitter, jitter_clip=3*small_jitter,\n",
    "                                         rot_deg=small_rot_deg, trans_mag=small_trans, scale_jitter=0.0)\n",
    "            x = torch.from_numpy(p_aug.T).unsqueeze(0).to(device)  # (1,3,N)\n",
    "            logits = model(x).squeeze(0)          # (C,N)\n",
    "            probs = torch.softmax(logits, dim=0)  # (C,N)\n",
    "            acc_probs = probs if acc_probs is None else (acc_probs + probs)\n",
    "        probs_avg = acc_probs / float(tta)\n",
    "        preds = probs_avg.argmax(dim=0).cpu().numpy()\n",
    "\n",
    "    # KNN 스무딩(정규화 좌표에서)\n",
    "    preds = knn_smooth(points, preds, k=knn_k)\n",
    "\n",
    "    # 원본 스케일 복원\n",
    "    out_xyz = points * s + c\n",
    "    out = np.hstack([out_xyz, np.array([inv_map[int(i)] for i in preds])[:, None]])\n",
    "    out_path = xyz_path.replace('.xyz', '_impeller_pred.xyzc')\n",
    "    np.savetxt(out_path, out, fmt='%.6f')\n",
    "    print(f\"[Saved] → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 사용 예시 (경로/파라미터는 상황에 맞게 수정)\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Train\n",
    "    train_impeller_robust(\n",
    "        xyzc_path=\"../assets/ply/impeller/3wings/impeller_b3_merged_all.xyzc\",\n",
    "        model_out=\"./models/impeller_seg_robust_b3.pth\",\n",
    "        lr=1e-4, stop_loss=0.001,\n",
    "        device=dev,\n",
    "        sample_size=2000,\n",
    "        lambda_consis=0.2,   # 일관성 가중치\n",
    "        beta_aug=0.2,        # 증강 배치 감독 가중치\n",
    "        epochs=2000,\n",
    "        use_label_smoothing=False,  # True로 바꾸면 CE + label smoothing 사용\n",
    "        label_smoothing=0.05,\n",
    "        use_ema_kld=False,   # True로 바꾸면 EMA Teacher + KL 일관성 사용\n",
    "        ema_mu=0.999, cons_temp=2.0,\n",
    "        save_curve=True\n",
    "    )\n",
    "\n",
    "    # Infer\n",
    "    infer_impeller_robust(\n",
    "        xyz_path=r\"C:\\Users\\user\\Documents\\GitHub\\Ai_coding_study\\point2cad\\assets\\xyz\\Impeller_b3_gpu.xyz\",\n",
    "        model_path=\"./models/impeller_seg_robust_b3.pth\",\n",
    "        device=dev,\n",
    "        tta=5, knn_k=16,\n",
    "        small_jitter=0.001, small_rot_deg=2, small_trans=0.003\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point2cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
