{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aea7b1f",
   "metadata": {},
   "source": [
    "### 데이터 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446de869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] casing01.ply → ..\\assets\\ply\\casing\\xyz\\casing01_point.xyz (N=10000)\n",
      "[OK] casing01.ply → ..\\assets\\ply\\casing\\xyz\\casing01_point.xyz (N=10000)\n",
      "[OK] casing02.ply → ..\\assets\\ply\\casing\\xyz\\casing02_point.xyz (N=10000)\n",
      "[OK] casing02.ply → ..\\assets\\ply\\casing\\xyz\\casing02_point.xyz (N=10000)\n",
      "[OK] casing03.ply → ..\\assets\\ply\\casing\\xyz\\casing03_point.xyz (N=10000)\n",
      "[OK] casing03.ply → ..\\assets\\ply\\casing\\xyz\\casing03_point.xyz (N=10000)\n",
      "[OK] casing04.ply → ..\\assets\\ply\\casing\\xyz\\casing04_point.xyz (N=10000)\n",
      "[OK] casing04.ply → ..\\assets\\ply\\casing\\xyz\\casing04_point.xyz (N=10000)\n",
      "[DONE] 총 8/8 개 파일 변환 완료.  (in: ..\\assets\\ply\\casing\\ply → out: ..\\assets\\ply\\casing\\xyz)\n"
     ]
    }
   ],
   "source": [
    "# ====== 설정 ======\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "BASE_DIR     = Path(r\"..\\assets\\ply\\casing\")\n",
    "INPUT_DIR    = BASE_DIR / \"ply\"   # ← PLY 읽는 폴더\n",
    "OUTPUT_DIR   = BASE_DIR / \"xyz\"   # ← XYZ 저장 폴더\n",
    "\n",
    "SUFFIX       = \"_point\"           # 저장 파일명 접미사 (원치 않으면 \"\")\n",
    "DOWNSAMPLE_N = 10000              # 포인트 샘플링 수\n",
    "np.random.seed(42)\n",
    "\n",
    "# ====== 변환 함수 ======\n",
    "def convert_ply_to_xyz(ply_path: Path, out_dir: Path, n_points: int = 1000, suffix: str = \"_point\"):\n",
    "    \"\"\"\n",
    "    단일 .ply → 점군(.xyz) 저장\n",
    "    - 삼각형 메쉬면 Poisson 디스크 샘플링\n",
    "    - 포인트클라우드면 랜덤 다운샘플\n",
    "    \"\"\"\n",
    "    # 1) 먼저 메쉬로 시도\n",
    "    mesh = o3d.io.read_triangle_mesh(str(ply_path))\n",
    "    pcd = None\n",
    "    if mesh is not None and mesh.has_vertices() and mesh.has_triangles():\n",
    "        try:\n",
    "            pcd = mesh.sample_points_poisson_disk(number_of_points=int(n_points))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] 메쉬 샘플링 실패 → 포인트클라우드로 재시도: {ply_path.name} ({e})\")\n",
    "\n",
    "    # 2) 메쉬가 아니거나 실패 시 포인트클라우드로 읽기\n",
    "    if pcd is None:\n",
    "        pc = o3d.io.read_point_cloud(str(ply_path))\n",
    "        pts = np.asarray(pc.points)\n",
    "        if pts.size == 0:\n",
    "            print(f\"[ERR] 포인트가 없습니다: {ply_path}\")\n",
    "            return None\n",
    "        if len(pts) > n_points:\n",
    "            idx = np.random.choice(len(pts), int(n_points), replace=False)\n",
    "            pts = pts[idx]\n",
    "        pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pts))\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"{ply_path.stem}{suffix}.xyz\"\n",
    "    np.savetxt(out_path, np.asarray(pcd.points), fmt=\"%.6f\")\n",
    "    print(f\"[OK] {ply_path.name} → {out_path} (N={len(pcd.points)})\")\n",
    "    return out_path\n",
    "\n",
    "# ====== 일괄 변환 ======\n",
    "def batch_convert(input_dir: Path, output_dir: Path, n_points: int = 1000, suffix: str = \"_point\"):\n",
    "    if not input_dir.is_dir():\n",
    "        print(f\"[ERR] 입력 폴더가 존재하지 않습니다: {input_dir}\")\n",
    "        return\n",
    "    ply_files = sorted(list(input_dir.glob(\"*.ply\")) + list(input_dir.glob(\"*.PLY\")))\n",
    "    if not ply_files:\n",
    "        print(f\"[INFO] PLY 파일이 없습니다: {input_dir}\")\n",
    "        return\n",
    "\n",
    "    cnt = 0\n",
    "    for ply in ply_files:\n",
    "        if convert_ply_to_xyz(ply, output_dir, n_points, suffix) is not None:\n",
    "            cnt += 1\n",
    "    print(f\"[DONE] 총 {cnt}/{len(ply_files)} 개 파일 변환 완료.  (in: {input_dir} → out: {output_dir})\")\n",
    "\n",
    "# 실행\n",
    "batch_convert(INPUT_DIR, OUTPUT_DIR, DOWNSAMPLE_N, SUFFIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96446bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[READ] casing01_point.xyz (N=10000)\n",
      "[READ] casing02_point.xyz (N=10000)\n",
      "[READ] casing03_point.xyz (N=10000)\n",
      "[READ] casing04_point.xyz (N=10000)\n",
      "[DONE] 4개 파일 병합 → ..\\assets\\ply\\casing\\xyz_merged\\imp_merged.xyz (총 포인트: 40,000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ====== xyz 파일 병합 및 imp_merged.xyz 생성 ======\n",
    "\n",
    "INPUT_DIR    = BASE_DIR / \"xyz\"   # ← PLY 읽는 폴더\n",
    "OUTPUT_DIR   = BASE_DIR / \"xyz_merged\"   # ← XYZ 저장 폴더\n",
    "\n",
    "xyz_files = sorted(INPUT_DIR.glob(\"*.xyz\"))\n",
    "if not xyz_files:\n",
    "    print(f\"[INFO] {INPUT_DIR} 안에 .xyz 파일이 없습니다.\")\n",
    "else:\n",
    "    merged_xyz = []\n",
    "    for f in xyz_files:\n",
    "        data = np.loadtxt(f)\n",
    "        if data.ndim == 1:\n",
    "            if data.size % 3 != 0:\n",
    "                print(f\"[WARN] {f.name} 포인트 형식 오류 → 스킵\")\n",
    "                continue\n",
    "            data = data.reshape(-1, 3)\n",
    "        merged_xyz.append(data)\n",
    "        print(f\"[READ] {f.name} (N={len(data)})\")\n",
    "    if merged_xyz:\n",
    "        merged_xyz = np.vstack(merged_xyz)\n",
    "        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        merged_path = OUTPUT_DIR / \"imp_merged.xyz\"\n",
    "        np.savetxt(merged_path, merged_xyz, fmt=\"%.6f\")\n",
    "        print(f\"[DONE] {len(xyz_files)}개 파일 병합 → {merged_path} (총 포인트: {len(merged_xyz):,d})\")\n",
    "    else:\n",
    "        print(\"[INFO] 병합할 데이터가 없습니다.\")\n",
    "\n",
    "#실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4814105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] shaft001_point.xyz → shaft001_point.xyzc (label=1.00000000)\n",
      "[OK] shaft002_point.xyz → shaft002_point.xyzc (label=2.00000000)\n",
      "[OK] shaft003_point.xyz → shaft003_point.xyzc (label=3.00000000)\n",
      "[DONE] 총 3개 파일 중 변환 성공 3개, 스킵 0개.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional   # ← 추가\n",
    "\n",
    "# ====== 경로 설정 ======\n",
    "BASE_DIR = Path(r\"..\\assets\\ply\\shaft\")\n",
    "IN_DIR   = BASE_DIR / \"xyz\"    # .xyz 읽기\n",
    "OUT_DIR  = BASE_DIR / \"xyzc\"   # .xyzc 저장\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== 3단계: .xyz 파일 → .xyzc 파일로 변환 ======\n",
    "def convert_xyz_to_xyzc(xyz_path: Path, out_dir: Path) -> Optional[Path]:  # ← 수정\n",
    "    \"\"\"\n",
    "    파일명에서 '마지막 숫자 묶음'을 라벨로 사용 → .xyzc 저장\n",
    "      impeller_b3_010_point.xyz -> 라벨 10.00000000\n",
    "      imp001_point.xyz          -> 라벨 1.00000000\n",
    "    \"\"\"\n",
    "    stem = xyz_path.stem\n",
    "    nums = re.findall(r'\\d+', stem)\n",
    "    if not nums:\n",
    "        print(f\"[SKIP] 라벨 숫자 추출 실패: {xyz_path.name}\")\n",
    "        return None\n",
    "\n",
    "    label_value = float(nums[-1])  # \"010\"->10.0, \"001\"->1.0\n",
    "\n",
    "    xyz = np.loadtxt(xyz_path)\n",
    "    if xyz.ndim == 1:\n",
    "        if xyz.size % 3 != 0:\n",
    "            print(f\"[ERR] 포인트 형식이 3열이 아닙니다: {xyz_path}\")\n",
    "            return None\n",
    "        xyz = xyz.reshape(-1, 3)\n",
    "\n",
    "    label_column = np.full((xyz.shape[0], 1), label_value, dtype=float)\n",
    "    xyzc = np.hstack((xyz, label_column))\n",
    "\n",
    "    out_path = out_dir / f\"{xyz_path.stem}.xyzc\"\n",
    "    np.savetxt(out_path, xyzc, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "    print(f\"[OK] {xyz_path.name} → {out_path.name} (label={label_value:.8f})\")\n",
    "    return out_path\n",
    "\n",
    "# ====== 폴더 내 모든 .xyz 처리 ======\n",
    "xyz_files = sorted(list(IN_DIR.glob(\"*_point.xyz\"))) or sorted(list(IN_DIR.glob(\"*.xyz\")))\n",
    "if not xyz_files:\n",
    "    print(f\"[INFO] {IN_DIR} 안에 처리할 .xyz 파일이 없습니다.\")\n",
    "else:\n",
    "    ok, skip = 0, 0\n",
    "    for xyz_file in xyz_files:\n",
    "        outp = convert_xyz_to_xyzc(xyz_file, OUT_DIR)\n",
    "        if outp is None: skip += 1\n",
    "        else: ok += 1\n",
    "    print(f\"[DONE] 총 {len(xyz_files)}개 파일 중 변환 성공 {ok}개, 스킵 {skip}개.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2702198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[READ] shaft001_point.xyzc (N=10000)\n",
      "[READ] shaft002_point.xyzc (N=10000)\n",
      "[READ] shaft003_point.xyzc (N=10000)\n",
      "[DONE] 3/3 개 파일 합침 → ..\\assets\\ply\\shaft\\xyzc\\merged_01.xyzc\n",
      "       총 포인트: 30,000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== 경로 설정 ======\n",
    "BASE_DIR = Path(r\"..\\assets\\ply\\shaft\")\n",
    "IN_DIR   = BASE_DIR / \"xyzc\"          # 병합할 개별 .xyzc들이 있는 폴더\n",
    "OUT_DIR  = BASE_DIR / \"xyzc\"   # 병합본 저장 폴더\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def next_merged_path(out_dir: Path, prefix: str = \"merged_\", start_idx: int = 1) -> Path:\n",
    "    \"\"\"\n",
    "    out_dir/prefix_XX.xyzc 형태로, 존재하지 않는 가장 낮은 인덱스를 찾아 경로 반환.\n",
    "    예) merged_01.xyzc, merged_02.xyzc, ...\n",
    "    \"\"\"\n",
    "    i = start_idx\n",
    "    while True:\n",
    "        candidate = out_dir / f\"{prefix}{i:02d}.xyzc\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# ====== 파일 수집 ======\n",
    "xyzc_files = sorted(IN_DIR.glob(\"*.xyzc\"))\n",
    "if not xyzc_files:\n",
    "    print(f\"[INFO] {IN_DIR} 안에 .xyzc 파일이 없습니다.\")\n",
    "else:\n",
    "    merged_chunks = []\n",
    "    n_files_read = 0\n",
    "    for f in xyzc_files:\n",
    "        try:\n",
    "            data = np.loadtxt(f).astype(np.float32)\n",
    "            if data.ndim == 1:\n",
    "                # 한 줄짜리 방지 및 4열 보장\n",
    "                if data.size % 4 != 0:\n",
    "                    print(f\"[WARN] 열 수가 4의 배수가 아님: {f.name} -> 스킵\")\n",
    "                    continue\n",
    "                data = data.reshape(-1, 4)\n",
    "            elif data.shape[1] != 4:\n",
    "                print(f\"[WARN] 4열(.xyzc)이 아님: {f.name} (shape={data.shape}) -> 스킵\")\n",
    "                continue\n",
    "\n",
    "            merged_chunks.append(data)\n",
    "            n_files_read += 1\n",
    "            print(f\"[READ] {f.name} (N={len(data)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {f.name} 불러오기 실패: {e}\")\n",
    "\n",
    "    if merged_chunks:\n",
    "        merged = np.vstack(merged_chunks)\n",
    "        out_path = next_merged_path(OUT_DIR, prefix=\"merged_\", start_idx=1)\n",
    "        np.savetxt(out_path, merged, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "        print(f\"[DONE] {n_files_read}/{len(xyzc_files)} 개 파일 합침 → {out_path}\")\n",
    "        print(f\"       총 포인트: {len(merged):,d}\")\n",
    "    else:\n",
    "        print(\"[INFO] 합칠 데이터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f3aac",
   "metadata": {},
   "source": [
    "여기부터는 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. 기본 설정 ===\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab88832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. 보조 함수 (정규화/증강 등) ===\n",
    "def normalize_points_xyz(xyz, mode=\"global\"):\n",
    "    \"\"\"\n",
    "    xyz: (N,3)\n",
    "    mode:\n",
    "      - \"global\": 평균 0, 표준편차 1 (등방 스케일)\n",
    "      - \"unit_box\": bbox의 가장 긴 변이 1이 되도록 스케일\n",
    "      - None: 그대로\n",
    "    \"\"\"\n",
    "    xyz = np.asarray(xyz, dtype=np.float32)\n",
    "    if mode is None:\n",
    "        return xyz, dict(mean=np.zeros(3), scale=1.0)\n",
    "    if mode == \"global\":\n",
    "        mean = xyz.mean(axis=0)\n",
    "        scale = xyz.std(axis=0).max()\n",
    "        scale = float(scale if scale > 0 else 1.0)\n",
    "        return (xyz - mean) / scale, dict(mean=mean, scale=scale)\n",
    "    if mode == \"unit_box\":\n",
    "        mn = xyz.min(axis=0); mx = xyz.max(axis=0)\n",
    "        extent = (mx - mn).max()\n",
    "        extent = float(extent if extent > 0 else 1.0)\n",
    "        mean = (mx + mn) / 2.0\n",
    "        return (xyz - mean) / extent, dict(mean=mean, scale=extent)\n",
    "    raise ValueError(\"unknown mode\")\n",
    "\n",
    "def jitter(xyz, sigma=0.005, clip=0.02):\n",
    "    noise = np.random.randn(*xyz.shape).astype(np.float32)  # ← float32 강제\n",
    "    noise = np.clip(noise * np.float32(sigma), -np.float32(clip), np.float32(clip))\n",
    "    return (xyz + noise).astype(np.float32)\n",
    "\n",
    "\n",
    "def rotate_z(xyz, deg=None):\n",
    "    if deg is None:\n",
    "        deg = np.random.uniform(0, 360)\n",
    "    th = np.deg2rad(deg)\n",
    "    R = np.array([[np.cos(th), -np.sin(th), 0.0],\n",
    "                  [np.sin(th),  np.cos(th), 0.0],\n",
    "                  [0.0,         0.0,        1.0]], dtype=np.float32)\n",
    "    return xyz @ R.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XYZCFiles(Dataset):\n",
    "    def __init__(self, files, normalize=\"global\", augment=False, sample_points=None):\n",
    "        \"\"\"\n",
    "        files: list[path-like] 여러 .xyzc 파일 경로\n",
    "        normalize: \"global\" | \"unit_box\" | None\n",
    "        augment: 학습 시 지터/회전 증강\n",
    "        sample_points: None이면 전체, 정수면 샘플링 개수\n",
    "        \"\"\"\n",
    "        self.files = [Path(f) for f in files]\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        self.sample_points = sample_points\n",
    "\n",
    "        # 1) 전체 로드\n",
    "        xyzes, labels = [], []\n",
    "        for f in self.files:\n",
    "            arr = np.loadtxt(f).astype(np.float32)\n",
    "            if arr.ndim == 1: arr = arr.reshape(-1, 4)\n",
    "            xyz, c = arr[:, :3], arr[:, 3]\n",
    "            xyzes.append(xyz); labels.append(c)\n",
    "        xyz_all = np.concatenate(xyzes, axis=0)\n",
    "        labels_all = np.concatenate(labels, axis=0)\n",
    "\n",
    "        # 2) 라벨 매핑(정렬/고유)\n",
    "        uniq = np.unique(labels_all)\n",
    "        self.lbl2idx = {v:i for i, v in enumerate(uniq)}   # 실제 라벨 → 0..K-1\n",
    "        self.idx2lbl = {i:v for v,i in self.lbl2idx.items()}\n",
    "        y_idx = np.vectorize(self.lbl2idx.get)(labels_all)\n",
    "\n",
    "        # 3) 정규화 파라미터 계산 (전 데이터 기준)\n",
    "        xyz_all_norm, stats = normalize_points_xyz(xyz_all, mode=normalize)\n",
    "        self.mean = stats[\"mean\"]; self.scale = stats[\"scale\"]\n",
    "\n",
    "        # 4) 파일별 인덱스 청크 저장(빠른 접근 용)\n",
    "        self.samples = []\n",
    "        offset = 0\n",
    "        for xyz, c in zip(xyzes, labels):\n",
    "            n = len(xyz)\n",
    "            idx_slice = slice(offset, offset+n)\n",
    "            # 실제 저장은 전체 풀에서 슬라이스로 참조\n",
    "            self.samples.append(idx_slice)\n",
    "            offset += n\n",
    "\n",
    "        # 5) 실제 풀 데이터 저장 (정규화 적용)\n",
    "        self.xyz_all = ((xyz_all - self.mean) / self.scale).astype(np.float32)\n",
    "        self.y_all = y_idx.astype(np.int64)\n",
    "\n",
    "        # 6) 클래스 가중치 (inverse frequency)\n",
    "        counts = np.bincount(self.y_all)\n",
    "        freq = counts / counts.sum()\n",
    "        self.class_weights = (1.0 / np.clip(freq, 1e-8, None)).astype(np.float32)\n",
    "        self.class_weights = self.class_weights * (len(self.class_weights) / self.class_weights.sum())\n",
    "\n",
    "        print(f\"[Dataset] 총 포인트: {len(self.xyz_all):,}, 클래스 수: {len(self.class_weights)}\")\n",
    "        print(\"[Dataset] 고유라벨(원본) → 인덱스:\", self.lbl2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 배치에 (B,3,N) 형태로 넣을거라 개별 포인트 샘플 보단\n",
    "        # '파일 단위' 아이템이 낫다. 파일 수를 길이로 보자.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sl = self.samples[i]\n",
    "        xyz = self.xyz_all[sl]\n",
    "        y = self.y_all[sl]\n",
    "\n",
    "        # 파일 단위에서 다시 포인트 샘플링 (선택)\n",
    "        if (self.sample_points is not None) and (len(xyz) > self.sample_points):\n",
    "            idx = np.random.choice(len(xyz), self.sample_points, replace=False)\n",
    "            xyz = xyz[idx]; y = y[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            xyz = rotate_z(xyz)  # 임펠러류에 유용한 z회전 불변성\n",
    "            xyz = jitter(xyz)\n",
    "\n",
    "        # (N,3), (N,) 반환 → collate에서 (B,N,3)로 쌓고 (B,3,N)로 전치\n",
    "        return torch.from_numpy(xyz), torch.from_numpy(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === collate_fn 정의 (파일 단위 배치를 한 덩어리로 합쳐서 반환) ===\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: [(xyz_i:(Ni,3), y_i:(Ni,)), ...] 길이 = batch_size(파일 수)\n",
    "    반환: xyz:(sumN,3), y:(sumN,)  -> 이후 학습 루프에서 (1,3,sumN) 형태로 변환\n",
    "    \"\"\"\n",
    "    xyzs = [b[0] for b in batch]\n",
    "    ys   = [b[1] for b in batch]\n",
    "    xyz  = torch.cat(xyzs, dim=0)   # (sumN, 3)\n",
    "    y    = torch.cat(ys,   dim=0)   # (sumN,)\n",
    "\n",
    "    # 섞어서 학습 안정화 (선택적)\n",
    "    perm = torch.randperm(xyz.size(0))\n",
    "    xyz  = xyz[perm]\n",
    "    y    = y[perm]\n",
    "    return xyz, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 데이터셋 생성 ===\n",
    "# 예) data_dir 안의 여러 *.xyzc 사용\n",
    "data_dir = r\"..\\assets\\ply\\impeller\\4wings\\xyzc_merged\"\n",
    "files = sorted(glob.glob(os.path.join(data_dir, \"*.xyzc\")))\n",
    "dataset = XYZCFiles(files, normalize=\"global\", augment=True, sample_points=40_000)\n",
    "\n",
    "\n",
    "# 학습/검증 분할(파일 단위)\n",
    "val_ratio = 0.1\n",
    "n_val = max(1, int(len(dataset) * val_ratio))\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "len(train_set), len(val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniPointNetSeg(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(3,    64, 1)\n",
    "        self.conv2 = nn.Conv1d(64,  128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256+64, 256, 1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.conv5 = nn.Conv1d(256, 128, 1)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        self.classifier = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):          # x: (B,3,N)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))      # (B,64,N)\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1)))     # (B,128,N)\n",
    "        x3 = F.relu(self.bn3(self.conv3(x2)))     # (B,256,N)\n",
    "        g  = torch.max(x3, dim=2, keepdim=True)[0]# (B,256,1) 전역맥스\n",
    "        g  = g.repeat(1, 1, x.size(2))            # (B,256,N)\n",
    "        x  = torch.cat([x1, g], dim=1)            # (B,64+256,N)\n",
    "        x  = F.relu(self.bn4(self.conv4(x)))      # (B,256,N)\n",
    "        x  = F.relu(self.bn5(self.conv5(x)))      # (B,128,N)\n",
    "        out = self.classifier(x)                  # (B,C,N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70acaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_FILES = 2            # 파일 2개씩 묶어서 한 배치 취급\n",
    "POINTS_PER_STEP = 80_000   # 학습 step마다 사용할 포인트 수(메모리 예산에 맞춰 조절)\n",
    "\n",
    "pin = (DEVICE.type == \"cuda\")\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_FILES, shuffle=True,\n",
    "                          num_workers=0, pin_memory=pin, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False,\n",
    "                          num_workers=0, pin_memory=pin, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.class_weights)\n",
    "model = MiniPointNetSeg(num_classes=num_classes).to(DEVICE)\n",
    "class_weights = torch.tensor(dataset.class_weights, dtype=torch.float32, device=DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "def random_pick_points(xyz, y, k):\n",
    "    n = xyz.size(0)\n",
    "    if n <= k:\n",
    "        idx = torch.arange(n, device=xyz.device)\n",
    "    else:\n",
    "        idx = torch.randperm(n, device=xyz.device)[:k]  # ← 중복 없음\n",
    "    return xyz[idx], y[idx]\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
    "    for xyz, y in train_loader:\n",
    "        xyz = xyz.to(DEVICE); y = y.to(DEVICE)\n",
    "        xyz, y = random_pick_points(xyz, y, POINTS_PER_STEP)\n",
    "\n",
    "        # (N,3) -> (1,3,N)\n",
    "        x = xyz.T.unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits = model(x)          # (1,C,N)\n",
    "            logits = logits.squeeze(0).T  # (N,C)로 변환\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc = (pred==y).float().mean().item()\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_acc  += acc * y.numel()\n",
    "        total_n    += y.numel()\n",
    "\n",
    "    print(f\"[Train {epoch}] loss={total_loss/total_n:.4f} acc={total_acc/total_n:.4f}\")\n",
    "\n",
    "def evaluate(epoch):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xyz, y in val_loader:\n",
    "            xyz = xyz.to(DEVICE); y = y.to(DEVICE)\n",
    "            xyz, y = random_pick_points(xyz, y, min(POINTS_PER_STEP, len(xyz)))\n",
    "            x = xyz.T.unsqueeze(0)\n",
    "            logits = model(x).squeeze(0).T\n",
    "            loss = criterion(logits, y)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc = (pred==y).float().mean().item()\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            total_acc  += acc * y.numel()\n",
    "            total_n    += y.numel()\n",
    "    print(f\"[Valid {epoch}] loss={total_loss/total_n:.4f} acc={total_acc/total_n:.4f}\")\n",
    "    return total_loss/total_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d956aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Confusion Matrix 유틸 ===\n",
    "import torch\n",
    "\n",
    "def update_confmat(cm: torch.Tensor, y_true: torch.Tensor, y_pred: torch.Tensor, num_classes: int):\n",
    "    \"\"\"\n",
    "    cm: (C,C) on DEVICE, long\n",
    "    y_true, y_pred: (N,) long, 값 범위 [0, C-1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.to(dtype=torch.long)\n",
    "    y_pred = y_pred.to(dtype=torch.long)\n",
    "    idx = y_true * num_classes + y_pred  # (N,)\n",
    "    # bincount은 long 필요 / minlength로 (C*C) 고정\n",
    "    binc = torch.bincount(idx, minlength=num_classes * num_classes)\n",
    "    cm += binc.view(num_classes, num_classes)\n",
    "    return cm\n",
    "\n",
    "def summarize_cm(cm: torch.Tensor):\n",
    "    \"\"\"\n",
    "    cm: (C,C) long/float on DEVICE\n",
    "    반환: dict(mean_iou, mean_class_acc, overall_acc, iou_per_class, acc_per_class)\n",
    "    \"\"\"\n",
    "    cm = cm.to(torch.float32)\n",
    "    tp = torch.diag(cm)                 # (C,)\n",
    "    fp = cm.sum(dim=0) - tp             # (C,)\n",
    "    fn = cm.sum(dim=1) - tp             # (C,)\n",
    "    denom_iou = tp + fp + fn + 1e-9\n",
    "    iou = tp / denom_iou                # (C,)\n",
    "\n",
    "    denom_acc = cm.sum(dim=1).clamp_min(1.0)\n",
    "    acc_cls = tp / denom_acc            # (C,)\n",
    "\n",
    "    mean_iou = iou.mean().item()\n",
    "    mean_acc = acc_cls.mean().item()\n",
    "    overall_acc = (tp.sum() / cm.sum().clamp_min(1.0)).item()\n",
    "\n",
    "    return {\n",
    "        \"mean_iou\": float(mean_iou),\n",
    "        \"mean_class_acc\": float(mean_acc),\n",
    "        \"overall_acc\": float(overall_acc),\n",
    "        \"iou_per_class\": iou.detach().cpu().numpy(),\n",
    "        \"acc_per_class\": acc_cls.detach().cpu().numpy(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time, os, pandas as pd, matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "def plot_history(hist):\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(hist[\"epoch\"], hist[\"train_loss\"], label=\"train\")\n",
    "    axes[0].plot(hist[\"epoch\"], hist[\"val_loss\"],   label=\"valid\")\n",
    "    axes[0].set_title(\"Loss\"); axes[0].legend(); axes[0].grid(True)\n",
    "    axes[1].plot(hist[\"epoch\"], hist[\"train_acc\"], label=\"train\")\n",
    "    axes[1].plot(hist[\"epoch\"], hist[\"val_acc\"],   label=\"valid\")\n",
    "    axes[1].set_title(\"Accuracy\"); axes[1].legend(); axes[1].grid(True)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def save_history_csv(hist, save_dir=\"./checkpoints_xyzc\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    pd.DataFrame(hist).to_csv(os.path.join(save_dir, \"training_history.csv\"), index=False)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "    it = tqdm(train_loader, desc=f\"Train {epoch}\", dynamic_ncols=True, leave=False, position=1)\n",
    "    for step, (xyz, y) in enumerate(it, 1):\n",
    "        xyz = xyz.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
    "        xyz, y = random_pick_points(xyz, y, POINTS_PER_STEP)\n",
    "        x = xyz.T.unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "            logits = model(x).squeeze(0).T\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            total_correct += (pred == y).sum().item()\n",
    "            total_loss    += loss.item() * y.numel()\n",
    "            total_n       += y.numel()\n",
    "\n",
    "        it.set_postfix({\n",
    "            \"step\": step,\n",
    "            \"avg_loss\": f\"{total_loss/max(1,total_n):.4f}\",\n",
    "            \"avg_acc\":  f\"{total_correct/max(1,total_n):.4f}\",\n",
    "            **({\"VRAM\": f\"{torch.cuda.memory_allocated()/(1024**3):.1f}G\"} if DEVICE.type==\"cuda\" else {})\n",
    "        })\n",
    "\n",
    "    return total_loss/max(1,total_n), total_correct/max(1,total_n)\n",
    "\n",
    "def evaluate(epoch, compute_cm=True):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "    cm = torch.zeros(num_classes, num_classes, dtype=torch.long, device=DEVICE) if compute_cm else None\n",
    "    it = tqdm(val_loader, desc=f\"Valid {epoch}\", dynamic_ncols=True, leave=False, position=2)\n",
    "    with torch.no_grad():\n",
    "        for step, (xyz, y) in enumerate(it, 1):\n",
    "            xyz = xyz.to(DEVICE, non_blocking=True); y = y.to(DEVICE, non_blocking=True)\n",
    "            xyz, y = random_pick_points(xyz, y, min(POINTS_PER_STEP, len(xyz)))\n",
    "            x = xyz.T.unsqueeze(0)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n",
    "                logits = model(x).squeeze(0).T\n",
    "                loss = criterion(logits, y)\n",
    "            pred = logits.argmax(dim=1)\n",
    "\n",
    "            total_correct += (pred == y).sum().item()\n",
    "            total_loss    += loss.item() * y.numel()\n",
    "            total_n       += y.numel()\n",
    "            if compute_cm: cm = update_confmat(cm, y, pred, num_classes)\n",
    "\n",
    "            it.set_postfix({\n",
    "                \"step\": step,\n",
    "                \"avg_loss\": f\"{total_loss/max(1,total_n):.4f}\",\n",
    "                \"avg_acc\":  f\"{total_correct/max(1,total_n):.4f}\",\n",
    "            })\n",
    "\n",
    "    stats = summarize_cm(cm) if (compute_cm and cm is not None) else None\n",
    "    return total_loss/max(1,total_n), total_correct/max(1,total_n), (cm.detach().cpu() if cm is not None else None), stats\n",
    "\n",
    "# === 학습 실행 ===\n",
    "EPOCHS   = 30000\n",
    "best_loss = float(\"inf\")\n",
    "save_dir  = \"./checkpoints_xyzc\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"pointnet_xyzc.pt\")\n",
    "\n",
    "last_cm = None\n",
    "last_stats = None\n",
    "\n",
    "outer = tqdm(range(1, EPOCHS+1), desc=\"Epochs\", dynamic_ncols=True, position=0, leave=True)\n",
    "for epoch in outer:\n",
    "    if DEVICE.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_acc, cm, stats = evaluate(epoch, compute_cm=True)\n",
    "\n",
    "    # 히스토리/CSV/그래프\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(val_loss);  history[\"val_acc\"].append(val_acc)\n",
    "    save_history_csv(history, save_dir=save_dir)\n",
    "    if epoch % 5 == 0: plot_history(history)\n",
    "\n",
    "    # best 갱신\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"lbl2idx\": dataset.lbl2idx,\n",
    "            \"idx2lbl\": dataset.idx2lbl,\n",
    "            \"norm\": {\"mean\": dataset.mean, \"scale\": dataset.scale},\n",
    "        }, save_path)\n",
    "        tqdm.write(f\"  ↳ best 갱신! 저장: {save_path}\")\n",
    "\n",
    "    last_cm, last_stats = cm, stats\n",
    "\n",
    "    if DEVICE.type == \"cuda\": torch.cuda.synchronize()\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    # ← 갱신된 best를 포함해 실시간 표시\n",
    "    outer.set_postfix({\n",
    "        \"train_loss\": f\"{tr_loss:.4f}\",\n",
    "        \"val_loss\":   f\"{val_loss:.4f}\",\n",
    "        \"train_acc\":  f\"{tr_acc:.4f}\",\n",
    "        \"val_acc\":    f\"{val_acc:.4f}\",\n",
    "        \"best\":       f\"{best_loss:.4f}\",\n",
    "        \"sec\":        f\"{dt:.1f}\"\n",
    "    })\n",
    "    tqdm.write(f\"[Epoch {epoch}] train_loss={tr_loss:.4f} val_loss={val_loss:.4f} (took {dt:.1f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if last_cm is not None and last_stats is not None:\n",
    "    cm = last_cm.numpy().astype(np.int64)\n",
    "    idx2lbl = dataset.idx2lbl\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "    plt.title(\"Confusion Matrix (Valid)\")\n",
    "    plt.xlabel(\"Pred\"); plt.ylabel(\"GT\")\n",
    "    plt.colorbar(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    iou = last_stats[\"iou_per_class\"]\n",
    "    classes = [idx2lbl[i] for i in range(len(iou))]\n",
    "    order = np.argsort(iou)\n",
    "    print(\"[Per-class IoU | bottom 5]\")\n",
    "    for i in order[:5]: print(f\"  class {classes[i]} -> IoU={iou[i]:.4f}\")\n",
    "    print(\"[Per-class IoU | top 5]\")\n",
    "    for i in order[-5:][::-1]: print(f\"  class {classes[i]} -> IoU={iou[i]:.4f}\")\n",
    "else:\n",
    "    print(\"[INFO] 아직 last_cm/last_stats가 없습니다. 학습 루프부터 실행하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c942ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 생성된 XYZ 파일로 테스트\n",
    "#xyz_test = str((Path(r\"..\\assets\\ply\\impeller\\4wings\\xyz\") / \"impeller_b3_010_point.xyz\"))\n",
    "#ckpt     = save_path\n",
    "#out_xyzc = str(Path(xyz_test).with_name(\"impeller_b3_010_pred.xyzc\"))\n",
    "#print(infer_xyz_to_xyzc(xyz_test, ckpt, out_path=out_xyzc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point2cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
