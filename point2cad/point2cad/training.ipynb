{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aea7b1f",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì…‹ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======1ë‹¨ê³„: ì…ë ¥ í´ë” ê²½ë¡œ ì„¤ì •\n",
    "path_in = r\"..\\assets\\ply\\impeller\\3wings\"   # Windows ê²½ë¡œ ì˜ˆì‹œ\n",
    "downsample_N = 20000         # ê°ê° í¬ì¸íŠ¸ ìƒ˜í”Œë§ ìˆ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446de869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from pathlib import Path\n",
    "\n",
    "#======= 2ë‹¨ê³„: ë‹¨ì¼ PLY íŒŒì¼ â†’ ì êµ°ìœ¼ë¡œ ë³€í™˜ í›„ .xyz ì €ì¥ =======\n",
    "def convert_to_xyz(path_in: str, downsample_N: int = 20000):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ PLY íŒŒì¼ â†’ ì êµ°ìœ¼ë¡œ ë³€í™˜ í›„ .xyz ì €ì¥\n",
    "    \"\"\"\n",
    "    mesh = o3d.io.read_triangle_mesh(path_in)\n",
    "    if mesh.has_triangles():\n",
    "        pcd = mesh.sample_points_poisson_disk(number_of_points=downsample_N)\n",
    "    else:\n",
    "        pcd = o3d.io.read_point_cloud(path_in)\n",
    "        if len(pcd.points) > downsample_N:\n",
    "            pts = np.asarray(pcd.points)\n",
    "            idx = np.random.choice(len(pts), downsample_N, replace=False)\n",
    "            pcd.points = o3d.utility.Vector3dVector(pts[idx])\n",
    "\n",
    "    out_path = Path(path_in).with_name(Path(path_in).stem + \"_point.xyz\")\n",
    "    np.savetxt(out_path, np.asarray(pcd.points), fmt=\"%.6f\")\n",
    "    print(f\"[OK] {path_in} â†’ {out_path} (N={len(pcd.points)})\")\n",
    "\n",
    "\n",
    "# ì§€ì •í•œ í´ë”ì— ìˆëŠ” ëª¨ë“  .ply íŒŒì¼ ë³€í™˜\n",
    "folder = Path(path_in)\n",
    "if not folder.is_dir():\n",
    "    print(f\"[ERR] í´ë” ê²½ë¡œê°€ ì•„ë‹˜: {folder}\")\n",
    "else:\n",
    "    ply_files = list(folder.glob(\"*.ply\"))\n",
    "    if not ply_files:\n",
    "        print(f\"[INFO] {folder} ì•ˆì— .ply íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        for ply in ply_files:\n",
    "            convert_to_xyz(str(ply), downsample_N=downsample_N)\n",
    "        print(f\"[DONE] ì´ {len(ply_files)} ê°œ íŒŒì¼ ë³€í™˜ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== 3ë‹¨ê³„: .xyz íŒŒì¼ â†’ .xyzc íŒŒì¼ë¡œ ë³€í™˜ ======\n",
    "def convert_xyz_to_xyzc(xyz_path: Path):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œ â†’ ë¼ë²¨ë¡œ ì¶”ê°€ â†’ .xyzc ì €ì¥\n",
    "    ì˜ˆ: impeller_b3_010_point.xyz â†’ ë¼ë²¨ = 10.00000000\n",
    "    \"\"\"\n",
    "    # íŒŒì¼ ì´ë¦„ì—ì„œ ìˆ«ì ì¶”ì¶œ\n",
    "    m = re.search(r\"_(\\d+)\", xyz_path.stem)\n",
    "    if not m:\n",
    "        print(f\"[SKIP] ë¼ë²¨ ìˆ«ì ì¶”ì¶œ ì‹¤íŒ¨: {xyz_path.name}\")\n",
    "        return None\n",
    "    \n",
    "    label_value = float(m.group(1))  # \"010\" â†’ 10.0\n",
    "\n",
    "    # xyz ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    xyz = np.loadtxt(xyz_path)\n",
    "    if xyz.ndim == 1:\n",
    "        xyz = xyz.reshape(-1, 3)\n",
    "\n",
    "    # ë¼ë²¨ ì¶”ê°€\n",
    "    label_column = np.full((xyz.shape[0], 1), label_value)\n",
    "    xyzc = np.hstack((xyz, label_column))\n",
    "\n",
    "    # ì €ì¥\n",
    "    out_path = xyz_path.with_suffix(\".xyzc\")\n",
    "    np.savetxt(out_path, xyzc, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "    print(f\"[OK] {xyz_path.name} â†’ {out_path.name} (label={label_value:.8f})\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ====== í´ë” ë‚´ ëª¨ë“  _point.xyz íŒŒì¼ ì²˜ë¦¬ ======\n",
    "folder = Path(path_in)\n",
    "xyz_files = list(folder.glob(\"*_point.xyz\"))\n",
    "\n",
    "if not xyz_files:\n",
    "    print(f\"[INFO] {folder} ì•ˆì— '_point.xyz' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    for xyz_file in xyz_files:\n",
    "        convert_xyz_to_xyzc(xyz_file)\n",
    "\n",
    "    print(f\"[DONE] ì´ {len(xyz_files)} ê°œ íŒŒì¼ ë³€í™˜ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b735c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== 4ë‹¨ê³„: ëª¨ë“  XYZC íŒŒì¼ ë³‘í•© ======\n",
    "\n",
    "output_file = Path(path_in) / \"merged_all.xyzc\"  # ìµœì¢… í•©ì³ì§„ íŒŒì¼ëª…\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  .xyzc ì°¾ê¸°\n",
    "folder = Path(path_in)\n",
    "xyzc_files = list(folder.glob(\"*.xyzc\"))\n",
    "\n",
    "if not xyzc_files:\n",
    "    print(f\"[INFO] {folder} ì•ˆì— .xyzc íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    merged_data = []\n",
    "    for f in xyzc_files:\n",
    "        try:\n",
    "            data = np.loadtxt(f)\n",
    "            if data.ndim == 1:  # í•œ ì¤„ì§œë¦¬ ë°©ì§€\n",
    "                data = data.reshape(1, -1)\n",
    "            merged_data.append(data)\n",
    "            print(f\"[READ] {f.name} (N={len(data)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {f.name} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    if merged_data:\n",
    "        merged_data = np.vstack(merged_data)\n",
    "        np.savetxt(output_file, merged_data, fmt=\"%.6f %.6f %.6f %.8f\")\n",
    "        print(f\"[DONE] {len(xyzc_files)} ê°œ íŒŒì¼ í•©ì¹¨ â†’ {output_file} (ì´ {len(merged_data)} í¬ì¸íŠ¸)\")\n",
    "    else:\n",
    "        print(\"[INFO] í•©ì¹  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_xyzc(path):\n",
    "    data = np.loadtxt(path)\n",
    "    if data.shape[1] != 4:\n",
    "        raise ValueError(\"Input file must have 4 columns: x, y, z, label\")\n",
    "    return data\n",
    "\n",
    "def print_layer_distribution(label_array):\n",
    "    total = len(label_array)\n",
    "    label_counts = Counter(label_array)\n",
    "    print(\"ğŸ“Š Layer distribution (by percentage):\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        percent = 100.0 * count / total\n",
    "        print(f\"  Layer {label:.6f}: {count} points ({percent:.2f}%)\")\n",
    "    print(\"\")\n",
    "\n",
    "def visualize_point_cloud(data):\n",
    "    x, y, z, label = data[:,0], data[:,1], data[:,2], data[:,3]\n",
    "    print_layer_distribution(label)\n",
    "\n",
    "    unique_labels = np.unique(label)\n",
    "    label_to_color = {l: plt.cm.tab20(i / max(len(unique_labels)-1, 1)) \n",
    "                      for i, l in enumerate(unique_labels)}\n",
    "    colors = np.array([label_to_color[l] for l in label])\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colors, s=1)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('Point Cloud Visualization by Layer')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load and visualize merged point cloud\n",
    "merged_xyzc_path = output_file  # already defined in your notebook\n",
    "data = load_xyzc(merged_xyzc_path)\n",
    "visualize_point_cloud(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708449f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Jupyter í•œ ì…€ì—ì„œ ëª¨ë‘ í•´ê²° ====\n",
    "# 1) í•„ìˆ˜ ì„í¬íŠ¸\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # <- Pylance 'optim' undefined í•´ê²°\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import mode\n",
    "\n",
    "def normalize_points(points):\n",
    "    # (N,3) â†’ ì›ì  ê¸°ì¤€, ë‹¨ìœ„ ìŠ¤ì¼€ì¼\n",
    "    c = points.mean(0)\n",
    "    p = points - c\n",
    "    s = np.max(np.linalg.norm(p, axis=1))\n",
    "    p = p / (s + 1e-8)\n",
    "    return p.astype(np.float32), (c, s)\n",
    "\n",
    "# ì¸ë±ìŠ¤ ë³´ì¡´í˜• ì¦ê°•(ë¼ë²¨ ì†ì‹¤ ì—†ìŒ): jitter/ì‘ì€ íšŒì „/ì´ë™/ìŠ¤ì¼€ì¼\n",
    "def aug_index_preserving(p, jitter_sigma=0.002, jitter_clip=0.01,\n",
    "                         rot_deg=2, trans_mag=0.005, scale_jitter=0.02):\n",
    "    q = p.copy()\n",
    "    # jitter\n",
    "    n = np.clip(np.random.normal(0, jitter_sigma, q.shape), -jitter_clip, jitter_clip)\n",
    "    q += n\n",
    "    # small rotation (zì¶• ì˜ˆì‹œ; í•„ìš”ì‹œ x/yë„ ì¶”ê°€)\n",
    "    th = np.deg2rad(np.random.uniform(-rot_deg, rot_deg))\n",
    "    c, s = np.cos(th), np.sin(th)\n",
    "    Rz = np.array([[c,-s,0],[s,c,0],[0,0,1]], np.float32)\n",
    "    q = (q @ Rz.T).astype(np.float32)\n",
    "    # small translation\n",
    "    t = np.random.uniform(-trans_mag, trans_mag, (1,3)).astype(np.float32)\n",
    "    q = q + t\n",
    "    # small scaling\n",
    "    sc = 1.0 + np.random.uniform(-scale_jitter, scale_jitter)\n",
    "    q = q * sc\n",
    "    return q\n",
    "\n",
    "def knn_smooth(points, preds, k=16):\n",
    "    k = int(min(k, len(points)))\n",
    "    idx = cKDTree(points).query(points, k=k)[1]\n",
    "    smoothed = mode(preds[idx], axis=1, keepdims=False).mode\n",
    "    return np.asarray(smoothed, dtype=preds.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_ROOT = os.path.abspath(\".\")  # ì˜ˆ: í”„ë¡œì íŠ¸ ë£¨íŠ¸\n",
    "if SRC_ROOT not in sys.path:\n",
    "    sys.path.append(SRC_ROOT)\n",
    "\n",
    "try:\n",
    "    # ë³´í†µ íŒŒì¼ ê²½ë¡œëŠ” src/PointNet_seg.py í˜•íƒœ\n",
    "    from src.PointNet_seg import PointNetSeg  # <- Pylance 'PointNetSeg' undefined í•´ê²°\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"PointNetSegë¥¼ importí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. \"\n",
    "        \"ë…¸íŠ¸ë¶ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ 'src/PointNet_seg.py'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , \"\n",
    "        \"ê²½ë¡œê°€ ë‹¤ë¥´ë©´ SRC_ROOTë¥¼ ë§ì¶°ì£¼ì„¸ìš”.\"\n",
    "    ) from e\n",
    "# 3) FocalLoss ì •ì˜ (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼/í˜¸í™˜)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (B,C) ë˜ëŠ” (N,C) í˜•íƒœ, targets: (B,) ë˜ëŠ” (N,)\n",
    "        ce = self.ce(logits, targets)             # (N,)\n",
    "        pt = torch.exp(-ce)                       # p = exp(-CE)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce      # focal\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "def train_impeller_robust(xyzc_path, model_out, lr, stop_loss, device, sample_size,\n",
    "                          lambda_consis=0.5, epochs=200):\n",
    "    print(f\"[Train-Robust] Using device: {device}\")\n",
    "    data = np.loadtxt(xyzc_path).astype(np.float32)\n",
    "    points = data[:, :3]\n",
    "    orig_labels = data[:, 3].astype(np.int64)\n",
    "\n",
    "    # 0) ì •ê·œí™”\n",
    "    points, _ = normalize_points(points)\n",
    "\n",
    "    # 1) ë¼ë²¨ ë§¤í•‘\n",
    "    unique_labels = np.unique(orig_labels)\n",
    "    label_to_idx = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    labels = np.array([label_to_idx[l] for l in orig_labels], dtype=np.int64)\n",
    "\n",
    "    # 2) ëª¨ë¸/ì†ì‹¤/ì˜µí‹°ë§ˆì´ì €\n",
    "    num_classes = len(unique_labels)\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "\n",
    "    # class-imbalance ê°€ì¤‘ì¹˜(ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    inv_freq = 1.0 / (counts + 1e-6)\n",
    "    weights = inv_freq / np.sum(inv_freq) * num_classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Focal + Consistency\n",
    "    criterion_sup = FocalLoss(gamma=2.0, weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    best_loss = float('inf'); best_state = None; full_loss_history = []\n",
    "    N = points.shape[0]; steps_per_epoch = max(1, N // sample_size)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        cycle_loss = 0.0\n",
    "        # ì ì§„ì  ë¨í”„ì—…(ì´ˆë°˜ì—” ê°ë… ìœ„ì£¼, í›„ë°˜ì—” ì¼ê´€ì„± ë¹„ì¤‘ â†‘)\n",
    "        lam = lambda_consis * min(1.0, epoch / (epochs*0.3))\n",
    "\n",
    "        for _ in range(steps_per_epoch):\n",
    "            idx = np.random.choice(N, sample_size, replace=False)\n",
    "            x_clean = points[idx]          # (B,3)\n",
    "            y_batch = labels[idx]          # (B,)\n",
    "\n",
    "            # ì¸ë±ìŠ¤ ìœ ì§€í•˜ëŠ” ì¦ê°•ë§Œ ì‚¬ìš©(ë¼ë²¨ ë§¤ì¹­ ê¹¨ì§€ì§€ ì•Šë„ë¡)\n",
    "            x_aug = aug_index_preserving(x_clean)\n",
    "\n",
    "            x_clean_t = torch.from_numpy(x_clean.T).unsqueeze(0).to(device)  # (1,3,B)\n",
    "            x_aug_t   = torch.from_numpy(x_aug.T).unsqueeze(0).to(device)\n",
    "            y_t       = torch.from_numpy(y_batch).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_clean = model(x_clean_t).squeeze(0).permute(1,0)  # (B,C)\n",
    "            logits_aug   = model(x_aug_t).squeeze(0).permute(1,0)    # (B,C)\n",
    "\n",
    "            # train ë£¨í”„ ë‚´ë¶€\n",
    "            loss_sup = criterion_sup(logits_clean, y_t)\n",
    "\n",
    "            beta = 0.2  # 0.1~0.3 ê¶Œì¥\n",
    "            loss_sup_aug = criterion_sup(logits_aug, y_t)\n",
    "\n",
    "            p_clean = F.softmax(logits_clean, dim=1)\n",
    "            p_aug   = F.softmax(logits_aug,   dim=1)\n",
    "            loss_cons = F.mse_loss(p_clean, p_aug)\n",
    "\n",
    "            loss = loss_sup + beta*loss_sup_aug + lam*loss_cons\n",
    "\n",
    "            loss.backward(); optimizer.step()\n",
    "            cycle_loss += float(loss.item())\n",
    "\n",
    "        avg_loss = cycle_loss / steps_per_epoch\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch}/{epochs}  Loss: {avg_loss:.6f}  (sup={loss_sup.item():.6f}, cons={loss_cons.item():.6f}, lam={lam:.3f})\")\n",
    "\n",
    "        # 10epochë§ˆë‹¤ full-loss(ì •ê·œí™”ëœ ì „ì²´ í¬ì¸íŠ¸ ê¸°ì¤€)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_full = torch.from_numpy(points.T).unsqueeze(0).to(device)\n",
    "                y_full = torch.from_numpy(labels).to(device)\n",
    "                logits_full = model(x_full).squeeze(0).permute(1,0)\n",
    "                full_loss = criterion_sup(logits_full, y_full).item()\n",
    "            model.train()\n",
    "            full_loss_history.append(full_loss)\n",
    "            print(f\"  FullLoss: {full_loss:.6f}\")\n",
    "\n",
    "            if full_loss < best_loss:\n",
    "                best_loss = full_loss; best_state = model.state_dict()\n",
    "                save_dir = os.path.dirname(model_out) or \".\"\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                torch.save({'state_dict': best_state, 'unique_labels': unique_labels.tolist()},\n",
    "                           os.path.join(save_dir, 'best_'+os.path.basename(model_out)))\n",
    "                print(f\"  [Best] FullLoss improved â†’ {best_loss:.6f}\")\n",
    "\n",
    "            # ê°„ë‹¨ early stop (ì›í•˜ë©´ ê²€ì¦ ì…‹ ë¶„ë¦¬ ì¶”ì²œ)\n",
    "            if len(full_loss_history)>=5 and all(l <= stop_loss for l in full_loss_history[-5:]):\n",
    "                print(f\"  [EarlyStop] last 5 FullLoss â‰¤ {stop_loss}\")\n",
    "                break\n",
    "\n",
    "    # ìµœì¢… ì €ì¥\n",
    "    os.makedirs(os.path.dirname(model_out), exist_ok=True)\n",
    "    final_state = best_state if best_state is not None else model.state_dict()\n",
    "    torch.save({'state_dict': final_state,\n",
    "                'unique_labels': unique_labels.tolist()}, model_out)\n",
    "    print(f\"[Saved] Model â†’ {model_out}  (best_loss={best_loss:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c18812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_impeller_robust(xyz_path, model_path, device, tta=5, knn_k=16,\n",
    "                          small_jitter=0.001, small_rot_deg=2, small_trans=0.003):\n",
    "    print(f\"[Infer-Robust] Using {device}\")\n",
    "    data = np.loadtxt(xyz_path).astype(np.float32)\n",
    "    orig_points = data[:, :3]                         # ì›ë³¸ ë³´ê´€\n",
    "    points, (c, s) = normalize_points(orig_points)    # ì •ê·œí™”\n",
    "\n",
    "    \n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    chk = torch.load(model_path, map_location=device)\n",
    "    inv_map = {i:l for i,l in enumerate(chk['unique_labels'])}\n",
    "    num_classes = len(chk['unique_labels'])\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(chk['state_dict']); model.eval()\n",
    "\n",
    "    # TTA: ì‘ì€ ë…¸ì´ì¦ˆ/íšŒì „/ì´ë™ ì—¬ëŸ¬ ë²ˆ â†’ í™•ë¥  í‰ê· \n",
    "    with torch.no_grad():\n",
    "        acc_probs = None\n",
    "        for _ in range(tta):\n",
    "            p_aug = aug_index_preserving(points,\n",
    "                                         jitter_sigma=small_jitter, jitter_clip=3*small_jitter,\n",
    "                                         rot_deg=small_rot_deg, trans_mag=small_trans, scale_jitter=0.0)\n",
    "            x = torch.from_numpy(p_aug.T).unsqueeze(0).to(device)\n",
    "            logits = model(x).squeeze(0)          # (C,N)\n",
    "            probs = torch.softmax(logits, dim=0)  # (C,N)\n",
    "            acc_probs = probs if acc_probs is None else (acc_probs + probs)\n",
    "        probs_avg = acc_probs / float(tta)\n",
    "        preds = probs_avg.argmax(dim=0).cpu().numpy()\n",
    "\n",
    "    # KNN ìŠ¤ë¬´ë”©\n",
    "    preds = knn_smooth(points, preds, k=knn_k)\n",
    "\n",
    "    # ì €ì¥ (ì›ë³¸ ìŠ¤ì¼€ì¼ ì¢Œí‘œë¥¼ ì“°ê³  ì‹¶ìœ¼ë©´ ì •ê·œí™” ì´ì „ ì¢Œí‘œ ë”°ë¡œ ë³´ê´€í•˜ì„¸ìš”)\n",
    "    out_xyz = points * s + c                          # ë³µì›!\n",
    "    out = np.hstack([out_xyz, np.array([inv_map[int(i)] for i in preds])[:, None]])\n",
    "    out_path = xyz_path.replace('.xyz','_impeller_pred.xyzc')\n",
    "    np.savetxt(out_path, out, fmt='%.6f'); print(f\"Saved â†’ {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_impeller_robust(\n",
    "    xyzc_path=\"../assets/ply/impeller/3wings/impeller_b3_merged_all.xyzc\",\n",
    "    model_out=\"./models/impeller_seg_robust_b3.pth\",\n",
    "    lr=1e-4, stop_loss=0.001,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    sample_size=2000,\n",
    "    lambda_consis=0.2, epochs=2000\n",
    ")\n",
    "\n",
    "# Infer\n",
    "infer_impeller_robust(\n",
    "    xyz_path=\"C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\Ai_coding_study\\\\point2cad\\\\assets\\\\xyz\\\\Impeller_b3_gpu.xyz\",\n",
    "    model_path=\"./models/impeller_seg_robust_b3.pth\",\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    tta=5, knn_k=16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Jupyter í•œ ì…€ì—ì„œ ëª¨ë‘ í•´ê²° (Robust PointNetSeg) ====\n",
    "# í•„ìˆ˜ ì„í¬íŠ¸\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import mode\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ìœ í‹¸\n",
    "# ------------------------------------------------------------\n",
    "def normalize_points(points: np.ndarray):\n",
    "    \"\"\"(N,3) â†’ ì›ì  ê¸°ì¤€ ë‹¨ìœ„ ìŠ¤ì¼€ì¼ ì •ê·œí™”, (ì •ê·œí™”ëœ ì , (ì¤‘ì‹¬, ìŠ¤ì¼€ì¼)) ë°˜í™˜\"\"\"\n",
    "    c = points.mean(0)\n",
    "    p = points - c\n",
    "    s = np.max(np.linalg.norm(p, axis=1))\n",
    "    p = p / (s + 1e-8)\n",
    "    return p.astype(np.float32), (c, s)\n",
    "\n",
    "def aug_index_preserving(p: np.ndarray,\n",
    "                         jitter_sigma=0.002, jitter_clip=0.01,\n",
    "                         rot_deg=2.0, trans_mag=0.005, scale_jitter=0.02):\n",
    "    \"\"\"ì¸ë±ìŠ¤ ë³´ì¡´í˜• ì¦ê°•(ë¼ë²¨ ì†ì‹¤ ì—†ìŒ)\"\"\"\n",
    "    q = p.copy()\n",
    "    # jitter\n",
    "    n = np.clip(np.random.normal(0, jitter_sigma, q.shape), -jitter_clip, jitter_clip)\n",
    "    q += n\n",
    "    # small rotation (zì¶•, í•„ìš” ì‹œ x/yë„ ì¶”ê°€ ê°€ëŠ¥)\n",
    "    th = np.deg2rad(np.random.uniform(-rot_deg, rot_deg))\n",
    "    c, s = np.cos(th), np.sin(th)\n",
    "    Rz = np.array([[c,-s,0],[s,c,0],[0,0,1]], np.float32)\n",
    "    q = (q @ Rz.T).astype(np.float32)\n",
    "    # small translation\n",
    "    t = np.random.uniform(-trans_mag, trans_mag, (1,3)).astype(np.float32)\n",
    "    q = q + t\n",
    "    # small scaling\n",
    "    sc = 1.0 + np.random.uniform(-scale_jitter, scale_jitter)\n",
    "    q = q * sc\n",
    "    return q\n",
    "\n",
    "def knn_smooth(points: np.ndarray, preds: np.ndarray, k=16):\n",
    "    \"\"\"KNN ë‹¤ìˆ˜ê²° ë¼ë²¨ ìŠ¤ë¬´ë”©\"\"\"\n",
    "    k = int(min(k, len(points)))\n",
    "    idx = cKDTree(points).query(points, k=k)[1]\n",
    "    smoothed = mode(preds[idx], axis=1, keepdims=False).mode\n",
    "    return np.asarray(smoothed, dtype=preds.dtype)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ëª¨ë¸ ì„í¬íŠ¸\n",
    "# ------------------------------------------------------------\n",
    "SRC_ROOT = os.path.abspath(\".\")  # ì˜ˆ: í”„ë¡œì íŠ¸ ë£¨íŠ¸\n",
    "if SRC_ROOT not in sys.path:\n",
    "    sys.path.append(SRC_ROOT)\n",
    "\n",
    "try:\n",
    "    # ë³´í†µ íŒŒì¼ ê²½ë¡œëŠ” src/PointNet_seg.py í˜•íƒœ\n",
    "    from src.PointNet_seg import PointNetSeg\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"PointNetSegë¥¼ importí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. \"\n",
    "        \"ë…¸íŠ¸ë¶ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ 'src/PointNet_seg.py'ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , \"\n",
    "        \"ê²½ë¡œê°€ ë‹¤ë¥´ë©´ SRC_ROOTë¥¼ ë§ì¶°ì£¼ì„¸ìš”.\"\n",
    "    ) from e\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì†ì‹¤ ì •ì˜\n",
    "# ------------------------------------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (N,C), targets: (N,)\n",
    "        ce = self.ce(logits, targets)             # (N,)\n",
    "        pt = torch.exp(-ce)                       # p = exp(-CE)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce      # focal\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (ì˜µì…˜) EMA Teacher & KL ì¼ê´€ì„±\n",
    "# ------------------------------------------------------------\n",
    "import copy\n",
    "def create_ema(model):\n",
    "    ema = copy.deepcopy(model).eval()\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return ema\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(student, teacher, mu=0.999):\n",
    "    for ps, pt in zip(student.parameters(), teacher.parameters()):\n",
    "        pt.data.mul_(mu).add_(ps.data, alpha=1-mu)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# í•™ìŠµ ë£¨í‹´\n",
    "# ------------------------------------------------------------\n",
    "def train_impeller_robust(xyzc_path,\n",
    "                          model_out=\"./models/impeller_seg_robust.pth\",\n",
    "                          lr=1e-4, stop_loss=0.001,\n",
    "                          device=None,\n",
    "                          sample_size=2000,\n",
    "                          lambda_consis=0.2,   # ì¼ê´€ì„± ì†ì‹¤ ê°€ì¤‘ì¹˜(ìµœëŒ€)\n",
    "                          beta_aug=0.2,        # ì¦ê°• ë°°ì¹˜ ê°ë… ê°€ì¤‘ì¹˜\n",
    "                          epochs=2000,\n",
    "                          use_label_smoothing=False, label_smoothing=0.05,\n",
    "                          use_ema_kld=False, ema_mu=0.999, cons_temp=2.0,\n",
    "                          save_curve=True):\n",
    "    \"\"\"\n",
    "    - ì¦ê°• ë°°ì¹˜ì—ë„ ê°ë… ì†ì‹¤ ì¶”ê°€(beta_aug)\n",
    "    - ì¼ê´€ì„± ì†ì‹¤ ë¨í”„ì—…(lambda_consis * min(1, epoch/(epochs*0.3)))\n",
    "    - (ì˜µì…˜) EMA teacher + KL ì¼ê´€ì„±\n",
    "    - CosineAnnealingLR ìŠ¤ì¼€ì¤„\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[Train-Robust] Using device: {device}\")\n",
    "\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    data = np.loadtxt(xyzc_path).astype(np.float32)\n",
    "    points = data[:, :3]\n",
    "    orig_labels = data[:, 3].astype(np.int64)\n",
    "\n",
    "    # ì •ê·œí™”\n",
    "    points, _ = normalize_points(points)\n",
    "\n",
    "    # ë¼ë²¨ ì¸ë±ìŠ¤í™”\n",
    "    unique_labels = np.unique(orig_labels)\n",
    "    label_to_idx = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "    labels = np.array([label_to_idx[l] for l in orig_labels], dtype=np.int64)\n",
    "\n",
    "    # ëª¨ë¸/ì†ì‹¤/ì˜µí‹°ë§ˆì´ì €\n",
    "    num_classes = len(unique_labels)\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "\n",
    "    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    inv_freq = 1.0 / (counts + 1e-6)\n",
    "    weights = inv_freq / np.sum(inv_freq) * num_classes\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    # ê°ë… ì†ì‹¤ ì„ íƒ\n",
    "    if use_label_smoothing:\n",
    "        criterion_sup = nn.CrossEntropyLoss(weight=class_weights,\n",
    "                                            label_smoothing=label_smoothing)\n",
    "    else:\n",
    "        criterion_sup = FocalLoss(gamma=2.0, weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # (ì˜µì…˜) EMA teacher\n",
    "    ema_model = create_ema(model) if use_ema_kld else None\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    full_loss_history = []\n",
    "\n",
    "    N = points.shape[0]\n",
    "    steps_per_epoch = max(1, int(np.ceil(N / max(1, sample_size))))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        cycle_loss = 0.0\n",
    "        # ì¼ê´€ì„± ë¨í”„ì—… (ì´ˆë°˜ 30% êµ¬ê°„)\n",
    "        lam = lambda_consis * min(1.0, epoch / (epochs * 0.3))\n",
    "\n",
    "        # (ì„ íƒ) í›„ë°˜ë¶€ ì•½ê°„ ì™„í™”\n",
    "        if epoch > 0.8 * epochs:\n",
    "            lam *= 0.5\n",
    "\n",
    "        # ì¦ê°• ê°•ë„ë„ í›„ë°˜ì— ì‚´ì§ ê°ì†Œ\n",
    "        cur_sigma = float(np.interp(epoch, [1, 0.7*epochs, epochs],\n",
    "                                    [0.002, 0.002, 0.001]))\n",
    "        cur_rot   = float(np.interp(epoch, [1, 0.7*epochs, epochs],\n",
    "                                    [2.0,   2.0,   1.0  ]))\n",
    "        cur_trans = 0.003  # ê³ ì • (í•„ìš”ì‹œ ìŠ¤ì¼€ì¤„ë§)\n",
    "\n",
    "        for _ in range(steps_per_epoch):\n",
    "            # ìƒ˜í”Œë§ (ë°ì´í„°ê°€ sample_sizeë³´ë‹¤ ì ìœ¼ë©´ replace=True)\n",
    "            replace_flag = sample_size > N\n",
    "            idx = np.random.choice(N, sample_size, replace=replace_flag)\n",
    "            x_clean = points[idx]      # (B,3)\n",
    "            y_batch = labels[idx]      # (B,)\n",
    "\n",
    "            # ì¸ë±ìŠ¤ ë³´ì¡´ ì¦ê°•\n",
    "            x_aug = aug_index_preserving(\n",
    "                x_clean,\n",
    "                jitter_sigma=cur_sigma, jitter_clip=3*cur_sigma,\n",
    "                rot_deg=cur_rot, trans_mag=cur_trans, scale_jitter=0.01\n",
    "            )\n",
    "\n",
    "            x_clean_t = torch.from_numpy(x_clean.T).unsqueeze(0).to(device)  # (1,3,B)\n",
    "            x_aug_t   = torch.from_numpy(x_aug.T).unsqueeze(0).to(device)\n",
    "            y_t       = torch.from_numpy(y_batch).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            logits_clean = model(x_clean_t).squeeze(0).permute(1, 0)  # (B,C)\n",
    "            logits_aug   = model(x_aug_t).squeeze(0).permute(1, 0)    # (B,C)\n",
    "\n",
    "            # ê°ë… ì†ì‹¤ (clean)\n",
    "            loss_sup = criterion_sup(logits_clean, y_t)\n",
    "\n",
    "            # ì¦ê°• ë°°ì¹˜ ê°ë…(ì•½í•˜ê²Œ)\n",
    "            loss_sup_aug = criterion_sup(logits_aug, y_t) if beta_aug > 0 else 0.0\n",
    "\n",
    "            # ì¼ê´€ì„± ì†ì‹¤\n",
    "            if use_ema_kld:\n",
    "                # EMA teacher KL( student||teacher )\n",
    "                with torch.no_grad():\n",
    "                    t_logits = ema_model(x_aug_t).squeeze(0).permute(1,0) / cons_temp\n",
    "                    t_prob   = F.softmax(t_logits, dim=1)\n",
    "                s_logits = logits_aug / cons_temp\n",
    "                log_prob = F.log_softmax(s_logits, dim=1)\n",
    "                loss_cons = F.kl_div(log_prob, t_prob, reduction='batchmean') * (cons_temp * cons_temp)\n",
    "            else:\n",
    "                # ë‹¨ìˆœ MSE(consistency)\n",
    "                p_clean = F.softmax(logits_clean, dim=1)\n",
    "                p_aug   = F.softmax(logits_aug,   dim=1)\n",
    "                loss_cons = F.mse_loss(p_clean, p_aug)\n",
    "\n",
    "            # ì´ì†ì‹¤\n",
    "            if isinstance(loss_sup_aug, float):\n",
    "                loss = loss_sup + lam * loss_cons\n",
    "            else:\n",
    "                loss = loss_sup + beta_aug * loss_sup_aug + lam * loss_cons\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # EMA ì—…ë°ì´íŠ¸\n",
    "            if ema_model is not None:\n",
    "                update_ema(model, ema_model, mu=ema_mu)\n",
    "\n",
    "            cycle_loss += float(loss.item())\n",
    "\n",
    "        avg_loss = cycle_loss / steps_per_epoch\n",
    "        scheduler.step()\n",
    "        # ëª¨ë‹ˆí„°ë§: ë§ˆì§€ë§‰ ìŠ¤í…ì˜ loss í•­ë“¤ ì¶œë ¥ (ëŒ€í‘œê°’)\n",
    "        cons_val = float(loss_cons.detach().cpu()) if torch.is_tensor(loss_cons) else float(loss_cons)\n",
    "        print(f\"Epoch {epoch}/{epochs}  Loss: {avg_loss:.6f}  (sup={float(loss_sup.detach().cpu()):.6f}, cons={cons_val:.6f}, lam={lam:.3f})\")\n",
    "\n",
    "        # 10 epochë§ˆë‹¤ ì „ì²´ í‰ê°€(ì •ê·œí™” ì¢Œí‘œ ê¸°ì¤€)\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_full = torch.from_numpy(points.T).unsqueeze(0).to(device)\n",
    "                y_full = torch.from_numpy(labels).to(device)\n",
    "                logits_full = model(x_full).squeeze(0).permute(1,0)\n",
    "                full_loss = criterion_sup(logits_full, y_full).item()\n",
    "            model.train()\n",
    "\n",
    "            full_loss_history.append(full_loss)\n",
    "            print(f\"  FullLoss: {full_loss:.6f}\")\n",
    "\n",
    "            # best ì €ì¥\n",
    "            save_dir = os.path.dirname(model_out) or \".\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            if full_loss < best_loss:\n",
    "                best_loss = full_loss\n",
    "                best_state = model.state_dict()\n",
    "                torch.save({'state_dict': best_state, 'unique_labels': unique_labels.tolist()},\n",
    "                           os.path.join(save_dir, 'best_'+os.path.basename(model_out)))\n",
    "                print(f\"  [Best] FullLoss improved â†’ {best_loss:.6f}\")\n",
    "\n",
    "            # ê°„ë‹¨ early stop\n",
    "            if len(full_loss_history) >= 5 and all(l <= stop_loss for l in full_loss_history[-5:]):\n",
    "                print(f\"  [EarlyStop] last 5 FullLoss â‰¤ {stop_loss}\")\n",
    "                break\n",
    "\n",
    "    # ìµœì¢… ì €ì¥\n",
    "    os.makedirs(os.path.dirname(model_out) or \".\", exist_ok=True)\n",
    "    final_state = best_state if best_state is not None else model.state_dict()\n",
    "    torch.save({'state_dict': final_state, 'unique_labels': unique_labels.tolist()}, model_out)\n",
    "    print(f\"[Saved] Model â†’ {model_out}  (best_loss={best_loss:.6f})\")\n",
    "\n",
    "    # Loss curve ì €ì¥(ì˜µì…˜)\n",
    "    if save_curve and len(full_loss_history) > 0:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(full_loss_history, marker='o')\n",
    "        plt.title('Full Loss History (every 10 epochs)')\n",
    "        plt.xlabel('Eval # (x10 epochs)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        curve_path = os.path.join(os.path.dirname(model_out) or \".\", \"full_loss_curve.png\")\n",
    "        plt.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"[Saved] Loss curve â†’ {curve_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì¶”ë¡  ë£¨í‹´\n",
    "# ------------------------------------------------------------\n",
    "def infer_impeller_robust(xyz_path, model_path, device=None,\n",
    "                          tta=5, knn_k=16,\n",
    "                          small_jitter=0.001, small_rot_deg=2, small_trans=0.003):\n",
    "    \"\"\"\n",
    "    - ì…ë ¥ xyzë¥¼ ì •ê·œí™” â†’ TTA(ì‘ì€ ì¦ê°• ì—¬ëŸ¬ ë²ˆ) í™•ë¥  í‰ê·  â†’ KNN ìŠ¤ë¬´ë”© â†’ ì›ë³¸ ìŠ¤ì¼€ì¼ ë³µì› â†’ xyzc ì €ì¥\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[Infer-Robust] Using {device}\")\n",
    "\n",
    "    data = np.loadtxt(xyz_path).astype(np.float32)\n",
    "    orig_points = data[:, :3]\n",
    "    points, (c, s) = normalize_points(orig_points)  # ì •ê·œí™” (í•œ ë²ˆë§Œ)\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    chk = torch.load(model_path, map_location=device)\n",
    "    inv_map = {i: l for i, l in enumerate(chk['unique_labels'])}\n",
    "    num_classes = len(chk['unique_labels'])\n",
    "    model = PointNetSeg(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(chk['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # TTA: ì‘ì€ ë…¸ì´ì¦ˆ/íšŒì „/ì´ë™ ì—¬ëŸ¬ ë²ˆ â†’ í™•ë¥  í‰ê· \n",
    "    with torch.no_grad():\n",
    "        acc_probs = None\n",
    "        for _ in range(tta):\n",
    "            p_aug = aug_index_preserving(points,\n",
    "                                         jitter_sigma=small_jitter, jitter_clip=3*small_jitter,\n",
    "                                         rot_deg=small_rot_deg, trans_mag=small_trans, scale_jitter=0.0)\n",
    "            x = torch.from_numpy(p_aug.T).unsqueeze(0).to(device)  # (1,3,N)\n",
    "            logits = model(x).squeeze(0)          # (C,N)\n",
    "            probs = torch.softmax(logits, dim=0)  # (C,N)\n",
    "            acc_probs = probs if acc_probs is None else (acc_probs + probs)\n",
    "        probs_avg = acc_probs / float(tta)\n",
    "        preds = probs_avg.argmax(dim=0).cpu().numpy()\n",
    "\n",
    "    # KNN ìŠ¤ë¬´ë”©(ì •ê·œí™” ì¢Œí‘œì—ì„œ)\n",
    "    preds = knn_smooth(points, preds, k=knn_k)\n",
    "\n",
    "    # ì›ë³¸ ìŠ¤ì¼€ì¼ ë³µì›\n",
    "    out_xyz = points * s + c\n",
    "    out = np.hstack([out_xyz, np.array([inv_map[int(i)] for i in preds])[:, None]])\n",
    "    out_path = xyz_path.replace('.xyz', '_impeller_pred.xyzc')\n",
    "    np.savetxt(out_path, out, fmt='%.6f')\n",
    "    print(f\"[Saved] â†’ {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ê²½ë¡œ/íŒŒë¼ë¯¸í„°ëŠ” ìƒí™©ì— ë§ê²Œ ìˆ˜ì •)\n",
    "# ------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Train\n",
    "    train_impeller_robust(\n",
    "        xyzc_path=\"../assets/ply/impeller/3wings/impeller_b3_merged_all.xyzc\",\n",
    "        model_out=\"./models/impeller_seg_robust_b3.pth\",\n",
    "        lr=1e-4, stop_loss=0.001,\n",
    "        device=dev,\n",
    "        sample_size=2000,\n",
    "        lambda_consis=0.2,   # ì¼ê´€ì„± ê°€ì¤‘ì¹˜\n",
    "        beta_aug=0.2,        # ì¦ê°• ë°°ì¹˜ ê°ë… ê°€ì¤‘ì¹˜\n",
    "        epochs=2000,\n",
    "        use_label_smoothing=False,  # Trueë¡œ ë°”ê¾¸ë©´ CE + label smoothing ì‚¬ìš©\n",
    "        label_smoothing=0.05,\n",
    "        use_ema_kld=False,   # Trueë¡œ ë°”ê¾¸ë©´ EMA Teacher + KL ì¼ê´€ì„± ì‚¬ìš©\n",
    "        ema_mu=0.999, cons_temp=2.0,\n",
    "        save_curve=True\n",
    "    )\n",
    "\n",
    "    # Infer\n",
    "    infer_impeller_robust(\n",
    "        xyz_path=r\"C:\\Users\\user\\Documents\\GitHub\\Ai_coding_study\\point2cad\\assets\\xyz\\Impeller_b3_gpu.xyz\",\n",
    "        model_path=\"./models/impeller_seg_robust_b3.pth\",\n",
    "        device=dev,\n",
    "        tta=5, knn_k=16,\n",
    "        small_jitter=0.001, small_rot_deg=2, small_trans=0.003\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "point2cad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
